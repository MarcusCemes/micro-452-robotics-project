{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "01a41824",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Mobile Robotics Project Report](#toc0_)\n",
    "\n",
    "_École Polytéchnique Fédérale de Lausanne_\n",
    "\n",
    "Marcus Cemes, Pablo Paller, Adrien Pannatier,  Carolina Rodrigues Fidalgo\n",
    "\n",
    "<div style=\"background-color: #f9fafb; border: 1px solid #d1d5db; border-radius: 0.25rem; padding: 1rem 2rem;\">\n",
    "    This notebook describes the different parts of our project for the course MICRO-452: Basics of mobile robotics.<br/>\n",
    "    In the following cells we will describe how the modules work and give a structure to run the project.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afed5a63",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Mobile Robotics Project Report](#toc1_)    \n",
    "  - [Prerequisites](#toc1_1_)    \n",
    "    - [Useful imports](#toc1_1_1_)    \n",
    "  - [Introduction](#toc1_2_)    \n",
    "    - [Features](#toc1_2_1_)    \n",
    "    - [Architecture](#toc1_2_2_)    \n",
    "    - [Useful concepts](#toc1_2_3_)    \n",
    "      - [The Signal](#toc1_2_3_1_)    \n",
    "      - [The Context](#toc1_2_3_2_)    \n",
    "      - [The State](#toc1_2_3_3_)    \n",
    "        - [State patches](#toc1_2_3_3_1_)    \n",
    "      - [The Module class](#toc1_2_3_4_)    \n",
    "      - [The Pool](#toc1_2_3_5_)    \n",
    "      - [The Web UI](#toc1_2_3_6_)    \n",
    "  - [Vision](#toc1_3_)    \n",
    "    - [Calibration](#toc1_3_1_)    \n",
    "    - [Frame analysis](#toc1_3_2_)    \n",
    "  - [Filtering](#toc1_4_)    \n",
    "    - [Prediction](#toc1_4_1_)    \n",
    "    - [Update](#toc1_4_2_)    \n",
    "  - [Global navigation](#toc1_5_)    \n",
    "        - [Time for some obstacles](#toc1_5_1_1_1_)    \n",
    "    - [Path optimisation](#toc1_5_2_)    \n",
    "  - [Big Brain](#toc1_6_)    \n",
    "    - [Significant updates](#toc1_6_1_)    \n",
    "    - [Christmas animation](#toc1_6_2_)    \n",
    "  - [Motion control](#toc1_7_)    \n",
    "        - [Responsibilities](#toc1_7_1_1_1_)    \n",
    "        - [Setup](#toc1_7_1_1_2_)    \n",
    "        - [Simulation](#toc1_7_1_1_3_)    \n",
    "  - [Local navigation](#toc1_8_)    \n",
    "  - [Putting it all together](#toc1_9_)    \n",
    "    - [Running from the terminal](#toc1_9_1_)    \n",
    "    - [Running from Jupyter](#toc1_9_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94048e0c",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Prerequisites](#toc0_)\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Tip:</b> In the latest Python/Jupyter version, it should no longer be necsesary to add our application module to the path. If you experience any import errors, convert the following block to code and run it before anything else.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c264085-9b4e-4ab4-b916-cdce9283d090",
   "metadata": {},
   "source": [
    "from sys import path\n",
    "path.append(\"./app\")\n",
    "path.append(\"./report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ebe28-e5a2-432d-9f98-9125c918c382",
   "metadata": {},
   "source": [
    "This project has been tested on **Python 3.10** and **JupyterLab 3.5**, allowing for features such as:\n",
    "\n",
    "- Python's new `match` statement\n",
    "- Top-level async/await\n",
    "- Integrated event loop in the notebook\n",
    "\n",
    "To make sure that your environment is supported, try running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c431f131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from report.utils import run_checks\n",
    "\n",
    "run_checks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b29f90",
   "metadata": {},
   "source": [
    "If you have any missing requirements, convert the following into a code block and run it (a [venv](https://docs.python.org/3/library/venv.html) is recommended)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "607d87d6-0b91-451f-8d29-d70634d9ad8d",
   "metadata": {},
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ed8216",
   "metadata": {},
   "source": [
    "Additional installation instructions can be found in the project's [README.md](README.md).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae13163",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_1_'></a>[Useful imports](#toc0_)\n",
    "\n",
    "We use these libraries and imports throughout the notebook, run this cell to make them available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a95436-6c5a-4c20-a856-f09d96e45597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "from IPython.display import clear_output\n",
    "from ipywidgets import interact\n",
    "\n",
    "from asyncio import create_task, sleep\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from app.config import *\n",
    "\n",
    "from report.map import *\n",
    "from report import report_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10079ba9",
   "metadata": {},
   "source": [
    "___\n",
    "## <a id='toc1_2_'></a>[Introduction](#toc0_)\n",
    "\n",
    "### <a id='toc1_2_1_'></a>[Features](#toc0_)\n",
    "\n",
    "- **Classes.** Each module is represented as a Python class, encapsulating useful logic and data and providing easy reference to `self` in event callbacks and asynchronous contexts.\n",
    "- **Singleton pattern.** There is only a single instance of each module class, handled by the Big Brain which gives orders.\n",
    "- **Shared context.** Modules are completely decoupled from each other. Instead, they share a `Context` object to communicate. This also simplified the report, modules can be instantiated and given new contexts for new scenarios, without dependence on global state.\n",
    "- **Signals.** Modules communicate via `Signal`s to trigger events. They are similar to semaphors in traditional multithreading.\n",
    "- **Module class.** Each module inherits from a `Module` class to simplify asynchronous behaviour and Thymio event callbacks.\n",
    "- **Control server & Web UI.** WebSocket-based web interface to interact with the application.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d4ccff8",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_2_'></a>[Architecture](#toc0_)\n",
    "\n",
    "Our project is composed by 8 different modules presented in the following figure:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738eb0d9-9262-4afe-8a98-cb2e187d1a34",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"padding: 1rem 0;\">\n",
    "    <figure>\n",
    "        <img src=\"assets/report/Project_scheme.jpg\" style=\"width: 40rem;\" />\n",
    "        <figcaption style=\"font-style: italic;\">Communication between the modules of the project.</figcaption>\n",
    "    </figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72854e05",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_3_'></a>[Useful concepts](#toc0_)\n",
    "\n",
    "#### <a id='toc1_2_3_1_'></a>[The Signal](#toc0_)\n",
    "\n",
    "The `Signal` class ([types.py](./app/utils/types.py))  is a custom asynchronous synchronisation primitive, it's used to allow modules to communicate that are running on the event loop, without having any dependence between each other. It wraps _asyncio_'s `Event` object, exposing two methods:\n",
    "\n",
    "- `async wait(timeout: float)`\n",
    "- `trigger()`\n",
    "\n",
    "When triggered, it will allow all other tasks that are `await`ing the signal to continue execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c430ab1d-6e38-41e2-919a-ab229cb52974",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of signalling\n",
    "\n",
    "from app.utils.types import Signal\n",
    "\n",
    "signal = Signal()\n",
    "\n",
    "async def wait_for_notification(id):\n",
    "    print(f\"Task {id} is waiting...\")\n",
    "    await signal.wait()\n",
    "    print(f\"Task {id} has been notified!\")\n",
    "\n",
    "# Create two tasks that will wait for the trigger\n",
    "for i in range(3):\n",
    "    create_task(wait_for_notification(i))\n",
    "\n",
    "await sleep(2)\n",
    "signal.trigger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e98c32c5",
   "metadata": {},
   "source": [
    "#### <a id='toc1_2_3_2_'></a>[The Context](#toc0_)\n",
    "\n",
    "One of the key elements in our project architecture is the `Context` class ([context.py](./app/context.py)). It allows us to to have a single shared point of access to important data, such as the mutable application state or to the connected Thymio nodes. It avoids the use of any global variables, and also allows us to create a simple mock interface for this notebook for demonstration purposes.\n",
    "\n",
    "Let's create one now that we will use for the rest of the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852ae7e7-d275-4a25-8105-792fff7fdf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.context import Context\n",
    "from app.state import State\n",
    "\n",
    "# We don't need a pool for this notebook, just run work synchronously\n",
    "from report.mock_pool import MockPool\n",
    "\n",
    "ctx = Context(node=None, node_top=None, pool=MockPool(), state=State())\n",
    "\n",
    "# Set configuration attributes from constants\n",
    "ctx.state.physical_size = PHYSICAL_SIZE_CM\n",
    "ctx.state.subdivisions = SUBDIVISIONS\n",
    "\n",
    "pprint(ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90852f6f-41df-4731-a6ab-eab33b70955b",
   "metadata": {},
   "source": [
    "It contains a few signals, that are used to decouple our modules from each other whilst allowing them to communicate events, the Thymio nodes, a shared state object and a few other items.\n",
    "\n",
    "| **Key**          | **Type**       | **Description**                                                            |\n",
    "|------------------|----------------|----------------------------------------------------------------------------|\n",
    "| **node**         | `Node`         | The primary Thymio node                                                    |\n",
    "| **node_top**     | `Node \\| None` | An optinal secondary Thymio node                                           |\n",
    "| **state**        | `State`        | The singleton `State` instance                                             |\n",
    "| **pool**         | `Pool`         | A ProcessPoolExecutor for offloading CPU-intensive work to another process |\n",
    "| **scene_update** | `Signal`       | A signal to indicate that the map has been changed                         |\n",
    "| **pose_update**  | `Signal`       | A signal to indicate that the robot has moved                              |\n",
    "| **debug_update** | `boolean`      | Queue a single iteration of vision image processing debugging (GUI)        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2768515d",
   "metadata": {},
   "source": [
    "#### <a id='toc1_2_3_3_'></a>[The State](#toc0_)\n",
    "\n",
    "The `State` class ([state.py](./app/state.py)) was originally created for a single purpose: simplify the update process to the Web UI. The `State` class internally has a `Signal` that is used to notify other tasks that the state has been modified, propagating the changes to all connected WebSocket clients over the control server.\n",
    "\n",
    "**Rule of thumb**: anything that is to be viewed on the Web UI should belong in state. Anything else can be a private class attribute.\n",
    "\n",
    "##### <a id='toc1_2_3_3_1_'></a>[State patches](#toc0_)\n",
    "\n",
    "The control server creates a `ChangeListener` for each WebSocket connection, which listens for state changes and creates a minimum patch from the last call. In the following demo, we create three listeners, each with variying amounts of \"busyness\". Each task is able to independently track changes to reach a consistent version of the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288a034d-dd0d-45b4-a5e7-3015ca4f4f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from report.introduction import scenario_state_patches\n",
    "\n",
    "await scenario_state_patches()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ef829a-580f-4abf-af39-a853cba6500c",
   "metadata": {},
   "source": [
    "> Understanding the implementation is not vital for this project. It allows for each WebSocket connection to have its own `ChangeListener`, with it's own dictionary of changes since the last call to `get_patch()`, ensuring that even if multiple clients are connected simultaneously, they can receive constient state updates. When a WebSocket connection is first made, the entire `State` is serialised and sent to the browser. This assited us in collaborative development and debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77792fa7",
   "metadata": {},
   "source": [
    "#### <a id='toc1_2_3_4_'></a>[The Module class](#toc0_)\n",
    "\n",
    "Our modules inherit from a custom `Module` class ([module.py](./app/utils/module.py)) that simplifies Thymio event processing and background asynchronous behaviour. The base class implements the `__enter__()` and `__exit__()` methods to integrate with Python's `with` statement.\n",
    "\n",
    "When the class enters the `with` context, it:\n",
    "\n",
    "- Checks if an `async run()` method is implemented, and spawns it in a new task\n",
    "- Creates a new Thymio event handler and registers the `process_event()` callback\n",
    "\n",
    "In the `process_event()` callback, it's possible to directly read the desired variable updates. If the current event does not contain the desired variables in the dictionary, a `KeyError` is raised and caught by the `Module` class.\n",
    "\n",
    "```python\n",
    "class UselessModule(Module):\n",
    "    \n",
    "    async def run(self):\n",
    "        \"\"\"This method will run in the background.\"\"\"\n",
    "        while True:\n",
    "            await self.ctx.pose_update.wait()\n",
    "            self.handle_pose_update()\n",
    "        \n",
    "    def process_event(self, variables):\n",
    "        \"\"\"This method is called every time the Thymio sends a variable event.\"\"\"\n",
    "        [vl] = variables[\"motor.left.speed\"]\n",
    "        [vr] = variables[\"motor.right.speed\"]\n",
    "        self.update_filtering(vl, vr)\n",
    "```\n",
    "\n",
    "When the `with` context is exited, the task is cancelled and the event handler is unregistered to free resources."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f56d9605",
   "metadata": {},
   "source": [
    "#### <a id='toc1_2_3_5_'></a>[The Pool](#toc0_)\n",
    "\n",
    "The pool is a wrapper around Python's `ProcessPoolExecutor`, simplifying the process of offloading expensive computation (such as pathfinding) to a different Python process that will not affect the current process' Global Interpreter Lock (multithreading is not sufficient for true parallelism).\n",
    "\n",
    "There is a little cost to serialising large data arrays, but ultimately makes the robot more reactive, even under load."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b04804ac",
   "metadata": {},
   "source": [
    "#### <a id='toc1_2_3_6_'></a>[The Web UI](#toc0_)\n",
    "\n",
    "To interact with our running application and to render a detailed map of detected obstacles, we developped a web application that connects to the Python application's control server on port 8080.\n",
    "\n",
    "It's use is not mandatory for this report, but can be used to connect to the application when it's run in its entirety at the end of this report. For more information, see the [README.md](./README.md). It requires a recent version of [Node.js®](https://nodejs.org) to be installed on your system.\n",
    "\n",
    "```powershell\n",
    "$ cd ui\n",
    "$ npm install\n",
    "$ npm run build\n",
    "$ npm run preview\n",
    "```\n",
    "\n",
    "<div align=\"center\" style=\"padding: 1rem 0;\">\n",
    "    <figure>\n",
    "        <img src=\"assets/report/ui.png\" style=\"width: 60rem;\" />\n",
    "        <figcaption style=\"font-style: italic;\">A screenshot of the Web UI with its various options</figcaption>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b> For the purposes of this report, we have developed a simplified view of the map that makes use of `matplotlib`. Throughout the report, we will make use of the plotting function to visualise the application state.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960784ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_map(ctx, \"Example map of the current state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f00da07d",
   "metadata": {},
   "source": [
    "___\n",
    "## <a id='toc1_3_'></a>[Vision](#toc0_)\n",
    "\n",
    "_Source: [vision.py](./app/vision.py)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7998ba31-d197-4a20-9767-f33693840e85",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset(ctx)\n",
    "\n",
    "from app.vision import Vision\n",
    "\n",
    "vision = Vision(ctx, external=False, live=False, image_path=\"assets/report/example_map.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc6d4c8c-3022-4c53-a931-a04d63ab743e",
   "metadata": {},
   "source": [
    "The first module we will go through is vision. Vision is the module responsible for acquiring information about the environment the Thymio is in. It has three main components, the actual map of obstacles and their position, the position of the Thymio and its orientation.\n",
    "\n",
    "Firstly, for the layout, to facilitate the image processing in vision we decided to use a white background where black obstacles are placed as well as the Thymio. The Thymio itself has two differently colored circles (orange and turquoise as these were the colors that provided the best contrast for detection) on top of it used for computing position and orientation of the robot. It's important to note that the center of the robot considered throughout the project corresponds to the center of the circle in the back of the Thymio.\n",
    "\n",
    "The vision class can be initialized with the following code, bearing in mind that the use of a live stream or previously recorded image is dependent on the boolean value of the variable \"live\", and in the case of the live stream, the source (either external or not to the machine running the code) can be defined in the \"external\" variable.\n",
    "\n",
    "Since using the camera is not interesting for our report, we will use the follwing image as an initial scene to demonstrate the module.\n",
    "\n",
    "<div align=\"center\" style=\"padding: 1rem 0;\">\n",
    "    <figure>\n",
    "        <img src=\"assets/report/example_map.png\" style=\"width: 30rem;\" />\n",
    "        <figcaption style=\"font-style: italic;\">Initial scene</figcaption>\n",
    "    </figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bddaab78",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_1_'></a>[Calibration](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b379af7e-bd03-4c4c-a3a4-45cfc5c8e0fa",
   "metadata": {},
   "source": [
    "The following step should be calibrating the image acquired by the camera (or loaded from a file).\n",
    "\n",
    "This method opens a window with the frame, in case of a live stream if the key 'N' is pressed a new frame will be captured and if 'S' is pressed the current frame will be saved. Now, in the following order, the user should click, using the left button of the mouse, the top left corner of the with square on the image, the top right corner, the bottom right corner, the bottom left corner, the circle in the back of the robot and finaly the circle in the from part of the Thymio.\n",
    "\n",
    "Starting with the corners, they will be used to do a simple perspective wrapping in order to focus only on that part of the image. The information from the circles is used to benchmark the values that codify the color that the camera perceives as their color.\n",
    "\n",
    "It's important to note that this method is only called the first time that vision is used given that it's assumed that the conditions like light and positioning of the camera won't change from then on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc89670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision.calibrate();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78393dad",
   "metadata": {},
   "source": [
    "### <a id='toc1_3_2_'></a>[Frame analysis](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e85f9cc-5260-4d4f-b0c7-b75778eec781",
   "metadata": {},
   "source": [
    "When the following method is called the capturing and analysis of a new frame is performed and the new observation is returned to be handled in big brain (which consists of the complete table of obstacles in a 64 by 64 matrix - best sizing for it to be used by global navigation - and Thymio position in centimeters).\n",
    "\n",
    "For the obstacle map, we detect the black pixels on the frame (obstacles), resize the frame to fit a 64 by 64 matrix and using a threshold atributte either 1 (obstacle present) or 0 (free place) to each entry of the matrix.\n",
    "\n",
    "For the two colored circles, their center is computed by, firstly, isolating them based on the color detected in calibration and afterwards calculating its convolution with a filter with the same diameter a the circle. The position where the convolution takes its maximum value is considered as the center.\n",
    "\n",
    "As mentioned earlier, the center of the back circle is used as the center of the robot. Orientation is computed using the center of both circles in big brain (not in vision since it should not be updated as often as the actual position, filtering is more reliable for that) ence why both values are returned."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b57cf45-3e58-4936-8856-e18419b0e6f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame = vision._read_image()\n",
    "frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "plot_image(frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d614f6-8bf4-49ca-9c44-48fdfb902f0f",
   "metadata": {},
   "source": [
    "In order to view the image processing steps in more detail, we can enable the debug flag in the context to allow the vision module to plot various stages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a15ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx.debug_update = True\n",
    "obs = vision.next()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f43858c-db52-4ce9-b6fe-4c690b959f57",
   "metadata": {},
   "source": [
    "_Top left: the captured image. Bottom left: the transformed image, with two dots indicating the detected landmark positions. Middle column: the landmark colour pixels. Right column: the landmark colour pixels after convolution with a circular kernel, the middle should be brightest (maximum)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2938f9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_pose = (obs.back[0], obs.back[1], report_functions.angle(obs.back, obs.front))\n",
    "print(vision_pose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939b1fc9",
   "metadata": {},
   "source": [
    "Now with the matplotlib map we can display the obstacles and the thymio pose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b79a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx.state.position = (obs.back[0], obs.back[1])\n",
    "ctx.state.orientation = report_functions.angle(obs.back, obs.front)\n",
    "ctx.state.obstacles = obs.obstacles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed55e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_map(ctx, \"Vision retranscription\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "498a2c0e-6e8c-4eef-995b-3d720c7675de",
   "metadata": {},
   "source": [
    "To compare it, here is the picture again:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b89e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_picture(\"assets/report/example_map.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6541631",
   "metadata": {},
   "source": [
    "___\n",
    "## <a id='toc1_4_'></a>[Filtering](#toc0_)\n",
    "\n",
    "_Source: [filtering.py](./app/filtering.py)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60f2d0c-fda5-489a-b1fb-f4588196d578",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset(ctx)\n",
    "\n",
    "from app.filtering import Filtering\n",
    "\n",
    "filtering = Filtering(ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9184e358",
   "metadata": {},
   "source": [
    "Another important module of our project is filtering. Filtering is taking the information given by sensors and the camera to be able to guess where the Thymio is on the map and where it's going.\n",
    "\n",
    "For this project we decieded to use an [Extanded Kalman Filter](app/EXF.py) and an intermediate module called [filtering](app/filtering.py) to comunicate with the main module.\n",
    "\n",
    "The EKF was chosen for its capability of predicting a non-linear evolution of the state and also to be able to dissociate information coming from the motors and information given by the camera.\n",
    "\n",
    "For this purpose, two functions where used: [predict](#toc1_4_1_) and [update](#toc1_4_2_). We will come back to these parts in the subsections below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79198013",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_functions.start_movement_simulation(filtering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d31dedb9",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_1_'></a>[Prediction](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0727adbe",
   "metadata": {},
   "source": [
    "A specificity of our project is the time delay between camera and motor sensors. Since the motor sensors response is faster than the one from vision, what we do is predict the position of the robot with the motor sensors at a greater frequency than the update with vision information. The time interval between each prediction is dynamic and adapts to correctly predict each time the motor sensors send variables.\n",
    "\n",
    "The next position and orientation are then predicted. Let us run it to see how it works.\n",
    "\n",
    "Let's take a look at our current position:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17073ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_functions.print_pose(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6efbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_map(ctx, \"Filtering Map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e8ca65-a9b1-4899-a710-2d010667779a",
   "metadata": {},
   "source": [
    "To execute a filtering step let's make the Thymio move forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7764f84c-156b-4b85-ae62-e9dec4ff37e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# We simulate a receieved motor speed event from the Thymio\n",
    "filtering.process_event({\"motor.left.speed\": [70], \"motor.right.speed\": [70]})\n",
    "\n",
    "report_functions.stop_movement_simulation(filtering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ca19d6-98c8-48a9-9395-84acfb11daf6",
   "metadata": {},
   "source": [
    "The state should now be updated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c973de18-b444-4774-ac4e-f7f84117f772",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_functions.print_pose(ctx)\n",
    "plot_map(ctx, \"Filtering Map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f19687a",
   "metadata": {},
   "source": [
    "Let's do it again, turning the robot this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5ccfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering.process_event({\"motor.left.speed\": [50], \"motor.right.speed\": [-50]})\n",
    "report_functions.stop_movement_simulation(filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98537fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_functions.print_pose(ctx)\n",
    "plot_map(ctx, \"Filtering Map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d77519",
   "metadata": {},
   "source": [
    "we see that filtering is correctly predicting the progression of the simulated robot."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9179907f",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_2_'></a>[Update](#toc0_)\n",
    "\n",
    "To reduce the error caused by motors, vision must be used as much as possible. If vision is lost for a while, the update step brings back the Thymio to it's correct position taking into consideration the computed prediction since it's disapearing.\n",
    "\n",
    "To display the update function, we use the image and position of the robot given by the [Vision module](#toc1_3_).\n",
    "\n",
    "The position after our prediction step is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155ea781",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_functions.print_pose(ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e43b13",
   "metadata": {},
   "source": [
    "now we update the position with the one given by vision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea0726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_position = vision #get image from vision\n",
    "filtering.update(vision_pose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb21370f",
   "metadata": {},
   "source": [
    "After the vision update, the new position is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b098a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_functions.print_pose(ctx)\n",
    "plot_map(ctx, \"Filtering Map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b33c5b01",
   "metadata": {},
   "source": [
    "___\n",
    "## <a id='toc1_5_'></a>[Global navigation](#toc0_)\n",
    "\n",
    "_Source: [global_navigation.py](./app/global_navigation.py)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fa2c9a-b7e0-4a09-8205-5c5f14549425",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset(ctx)\n",
    "\n",
    "from app.global_navigation import GlobalNavigation\n",
    "\n",
    "global_nav = GlobalNavigation(ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20272c96-5932-449e-bd7e-0aa6dd99bbdb",
   "metadata": {},
   "source": [
    "The global navigation module is tasked with pathfinding. More specifically, this means computing the optimal path in order to reach the destination from the Thymio's position whilst staying clear of any obstacles picked up by the vision module.\n",
    "\n",
    "This path is then communicated to motion control via the shared state which will then proceed to follow that path whilst giving priority to local navigation and its obstacle avoidance capabilities.\n",
    "\n",
    "At the moment our map is completely empty. In order to invoke our pathfinding algorithm, we need to set the Thymio's position (<b style=\"color: #dc2626;\">red</b>) and a desired end point (<b style=\"color: #0891b2;\">cyan</b>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031257b3-5db9-4211-a549-bbf6ca99c3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the state\n",
    "ctx.state.position = (10, 10)\n",
    "ctx.state.end = (90, 110)\n",
    "\n",
    "# Recompute the path, updating the state\n",
    "await global_nav._recompute_path()\n",
    "\n",
    "# Plot the result\n",
    "plot_map(ctx, \"Pathfinding result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de96a048",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "\n",
    "**Note:** As `_recompute_path()` is prefixed with an underscore, it is inferred that it is a private class method. For the purpose of this report, we will use it without shame to update our `State` with a new pathfinding computation. Under normal operation, the `ctx.scene_update` signal can be simply triggered to queue a new path recomputation using the pool.\n",
    "\n",
    "</div>\n",
    "\n",
    "**🎉 We have a path!**\n",
    "\n",
    "Let's not celebrate yet, we can make this much more interesting. Our pathfinding uses an implementation of the [Dijkstra's algorithm](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm). While this may not be the fastest or most efficient algorithm, it _does_ guarantee optimal results for a given graph.\n",
    "\n",
    "Our node graph is a grid of equaly spaced cells, where edges are made between directly adjacent cells and diagonal cells, with a cost of 1 and $\\sqrt{2}$, respectively. This allows our robot to also travel diagonally and avoid grid-like movement.\n",
    "\n",
    "<div align=\"center\" style=\"padding: 1rem 0;\">\n",
    "    <figure>\n",
    "        <img src=\"assets/report/dijkstra-nodes.png\" style=\"width: 8rem;\" />\n",
    "        <figcaption style=\"font-style: italic;\">Edges for a graph node to its adjacent nodes.</figcaption>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "Ideally, we would like all nodes to be connected to every other node, given that there is a line-of-sight between them, however this comes with a huge computational burden. This simple implementation (mixed with our path optimisation algorithm covered later) provides adequate results in roughly **50 ms** for 64 subdivisions.\n",
    "\n",
    "##### <a id='toc1_5_1_1_1_'></a>[Time for some obstacles](#toc0_)\n",
    "\n",
    "Obstacles come directly from the vision module as a matrix of 8-bit integers, where any value other an 0 represent an occupied cell. The size of the matrix coincides with the node graph, an occupied cell will mark a node as unvisitable.\n",
    "\n",
    "> While we could have also used a matrix of booleans, integers are slightly easier to work with and compose with other functions such as convolutions. Additionally, it has a smaller JSON serialisation footprint for sending over WebSockets to the Web UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce68b7b-0da6-4a1f-9831-4164dcae34fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the matrix\n",
    "ctx.state.obstacles[:, :] = 0\n",
    "\n",
    "# Add two obstacles\n",
    "ctx.state.obstacles[30:45, 40:55] = 1\n",
    "ctx.state.obstacles[20:30, 5: 20] = 1\n",
    "\n",
    "# Recompute the path\n",
    "await global_nav._recompute_path()\n",
    "\n",
    "# Plot the result\n",
    "plot_map(ctx, \"Pathfinding result\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ad200-eee1-4a3d-9148-760c033a7b21",
   "metadata": {},
   "source": [
    "The <b style=\"color: #075985\">dark-blue</b> region represents the obstacles that we set. The <b style=\"color: #0891b2\">cyan</b> regions are the obstacle boundaries that are generated using a convolution of a circular kernel matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f01aae-80a6-4476-b9dd-287c0007d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = global_nav._safety_margin_kernel()\n",
    "\n",
    "plot_image(kernel, \"Obstacle boundary convolution kernel\", colourbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916d9a38-5988-44e6-a868-db8a058f38e0",
   "metadata": {},
   "source": [
    "The boundary is equaly treated as unvisitable by the pathfinding algorith. This ensures that the Thymio does not hit any objects with its wheels as it's navigating the path. It a gap is too small, the Thymio will not try to fit through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc68a70-6455-42bf-a33f-f6299aeeb3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the matrix\n",
    "ctx.state.obstacles[:,:] = 0\n",
    "\n",
    "# Add two obstacles with a gap\n",
    "ctx.state.obstacles[31:33, 15:29] = 1\n",
    "ctx.state.obstacles[31:33, 42:50] = 1\n",
    "\n",
    "# Recompute the path\n",
    "await global_nav._recompute_path()\n",
    "\n",
    "# Plot the result\n",
    "plot_map(ctx, \"Path-finding map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93d810d4-357c-4f8b-8fed-1f51a601402a",
   "metadata": {},
   "source": [
    "_A gap is clearly visible between the obstacles, however the pathfinding algorithm instead is forced to take a longer path around both obstacles._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab6f2b4b",
   "metadata": {},
   "source": [
    "### <a id='toc1_5_2_'></a>[Path optimisation](#toc0_)\n",
    "\n",
    "Our chosen node graph constraints our robots movements more or less to a grid, with the ability of also being able to move diagonally. This creates a lot of path waypoints, but also creates less natural-like movement.\n",
    "\n",
    "To combat this, we have an additional post-pathfinding step to iteratively try to reduce the path to as few waypoints as necessary that have free line of sight between them. This does not find the most optimal solution, but the results are more than adequate.\n",
    "\n",
    "The algorithm goes as follows:\n",
    "\n",
    "1. For $i = 2,...,M-1$, where $M$ is the number of waypoints\n",
    "2. Check whether there is a free line-of-sight between waypoints $w_{i-1}$ and $w_{i+1}$\n",
    "3. If yes, remove the waypoint $w_i$, otherwise consider the next waypoint $w_{i+1}$\n",
    "\n",
    "In order to calculate free line-of-sight, it's necessary to enumerate all map cells that the segment between $w_{i-1}$ and $w_{i+1}$ travels through to check their occupation status. This is done using an algorithm such as [Bresenham's line algorithm](https://en.wikipedia.org/wiki/Bresenham%27s_line_algorithm). We found an improved version that also uses integer-only math [here](https://playtechs.blogspot.com/2007/03/raytracing-on-grid.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bb9c40-6fda-40c1-8730-e0a7f0f7c175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.path_finding.path_optimiser import raytrace\n",
    "\n",
    "bounds = (0, 9, 1)\n",
    "\n",
    "@interact(y1=bounds, y2=bounds)\n",
    "def plot(y1=2, y2=6):\n",
    "    p1, p2 = (1, y1), (8, y2)\n",
    "\n",
    "    matrix = np.zeros((10, 10))\n",
    "\n",
    "    for (i, j) in raytrace(p1, p2):\n",
    "        matrix[j][i] = 1\n",
    "\n",
    "    plot_raytrace(matrix, p1, p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e684f7c-1e87-4298-ad7e-3fd33f89b28b",
   "metadata": {},
   "source": [
    "Let's put this into practice using the path that we found previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a91d176-ac3b-42e9-88c7-175818e8c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.path_finding.path_optimiser import PathOptimiser\n",
    "\n",
    "map = create_map(ctx)\n",
    "\n",
    "optimiser = PathOptimiser(map)\n",
    "\n",
    "# The output of the pathfinding algorithm is in physical space [cm],\n",
    "# it needs to be converted back into map indicies for this demonstration.\n",
    "# Usually the path optimisation is done as an intermediate step by the\n",
    "# path finding algorithm, before the conversion to physical space.\n",
    "path = path_to_coords(ctx.state.path)\n",
    "\n",
    "# The optimiser mutates the input list directly\n",
    "optimised_path = optimiser.optimise(path.copy())\n",
    "\n",
    "plot_path_optimisation(path, optimised_path, map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1ad1fd-2f4a-4207-b794-703dd6b9daa9",
   "metadata": {},
   "source": [
    "Path optimisation can be toggled dynamically by setting the `ctx.state.optimise` boolean flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca233e2-1175-489f-96dc-6dee946cdac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable path optimisation\n",
    "ctx.state.optimise = True\n",
    "\n",
    "# Reset the matrix\n",
    "ctx.state.obstacles[:,:] = 0\n",
    "\n",
    "# Add two obstacles\n",
    "ctx.state.obstacles[30:45, 40:55] = 1\n",
    "ctx.state.obstacles[20:30, 5: 20] = 1\n",
    "\n",
    "await global_nav._recompute_path()\n",
    "\n",
    "plot_map(ctx, \"Path-finding map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9edb3d3c",
   "metadata": {},
   "source": [
    "___\n",
    "## <a id='toc1_6_'></a>[Big Brain](#toc0_)\n",
    "\n",
    "_Source: [big_brain.py](./app/big_brain.py)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd21e289",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset(ctx)\n",
    "\n",
    "import app.big_brain\n",
    "\n",
    "big_brain = app.big_brain.BigBrain(ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10a309a",
   "metadata": {},
   "source": [
    "The Big Brain module is in charge of the main application logic. It's purpose is to initialise the other modules and to orchestrate the flow of data between them.\n",
    "\n",
    "The submodules are able to run autonomously in the background of the application using Python's `with` statement and the `asyncio` library. The `__enter__` and `__exit__` methods are used to initialise and clean up the resources used by the submodules, such as long running tasks or registering and unregistering Thymio event handlers.\n",
    "\n",
    "Once the submodules are initialised, the main loop of the application is started. This loop is responsible for acquiring new data from the vision module and providing this data to the filtering and global navigation modules, which then trickle down to motion control and to the Web UI.\n",
    "\n",
    "### <a id='toc1_6_1_'></a>[Significant updates](#toc0_)\n",
    "\n",
    "To reduce the number of map updates from the vision module, we have implemented a simple thresholding function that checks whether the $L_1$ norm between the current and previous map is greater than a given threshold (i.e. a certain number of grid squares have changed). This reduces redundant pathfinding computation and also renders motion control more stable, as it is less likely to be interrupted by a sudden change in the map and path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c56636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx.state.obstacles = np.zeros((SUBDIVISIONS, SUBDIVISIONS), dtype=np.int8)\n",
    "new_map = np.zeros((SUBDIVISIONS, SUBDIVISIONS), dtype=np.int8)\n",
    "\n",
    "# Changing a single square does not report a significant change\n",
    "new_map[1, 1] = 1\n",
    "\n",
    "big_brain.significant_change(new_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d36e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing more than the threshold of squares should report a significant change\n",
    "new_map[1:5, 1:5] = 1\n",
    "\n",
    "big_brain.significant_change(new_map, threshold=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb73eed",
   "metadata": {},
   "source": [
    "If the new map is deemed to have a significant change, the obstacle map is updated in `ctx.state` and the `ctx.scene_update` signal is triggered, which in turn triggers the global navigation module to recompute the path.\n",
    "\n",
    "Regardless of whether the map has changed or not, the new detected position is sent to the filtering module in order to update the robot's position and attempt to correct any drift.\n",
    "\n",
    "### <a id='toc1_6_2_'></a>[Christmas animation](#toc0_)\n",
    "\n",
    "Once the robot reaches the arrival point, it will start to play a celebration sequence. This is also handled by the big brain module, invoking the `ChristmasCelebration` class to play various actions, such as dropping the bauble using the second Thymio node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f44f9ef4",
   "metadata": {},
   "source": [
    "## <a id='toc1_7_'></a>[Motion control](#toc0_)\n",
    "\n",
    "_Source: [motion_contor.py](./app/motion_control.py)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7717523",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset(ctx)\n",
    "\n",
    "from app.motion_control import MotionControl\n",
    "\n",
    "control = MotionControl(ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd021776",
   "metadata": {},
   "source": [
    "The motion control block has the responsability to move the Thymio. It has two types of control; positional control `controlPosition()` which has the objective to go to a special set of coordinates and reactive control `controlWithDistance()` which has the objective to avoid any walls and get around them following their left.\n",
    "\n",
    "Motion control also has the responsability to update the targeted waypoint if it has been reached and commmunicate to the rest of the program if the last waypoint is reached.\n",
    "\n",
    "##### <a id='toc1_7_1_1_1_'></a>[Responsibilities](#toc0_)\n",
    "\n",
    "1. Update the waypoint when arrived\n",
    "2. If arrived to the end of the path, change the state to let the rest of the program know\n",
    "\n",
    "Let's simulate the robot following a path!\n",
    "\n",
    "##### <a id='toc1_7_1_1_2_'></a>[Setup](#toc0_)\n",
    "\n",
    "1. We have do define a path (noramlly given by BigBrain in Global-nav)\n",
    "2. We have to define which waypoint we are aiming\n",
    "\n",
    "\n",
    "##### <a id='toc1_7_1_1_3_'></a>[Simulation](#toc0_)\n",
    "\n",
    "1. We control the Thymio to go to the specified waypoint we want \n",
    "2. We update his position by updating filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6beff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_functions.start_movement_simulation(filtering)\n",
    "\n",
    "reset(ctx)\n",
    "\n",
    "# Set some locations in the state\n",
    "ctx.state.position = (25, 18)\n",
    "ctx.state.orientation = 0\n",
    "ctx.state.end = (53, 55)\n",
    "ctx.state.reactive_control = False\n",
    "\n",
    "# Setup the filtering module\n",
    "filtering.update((*ctx.state.position, ctx.state.orientation))\n",
    "\n",
    "# Reset the matrix\n",
    "ctx.state.obstacles[:,:] = 0\n",
    "\n",
    "# Add two obstacles\n",
    "ctx.state.obstacles[30:45, 40:55] = 1\n",
    "ctx.state.obstacles[20:30, 5: 20] = 1\n",
    "\n",
    "# Recompute the path\n",
    "await global_nav._recompute_path()\n",
    "\n",
    "# Setup the motion control module\n",
    "ctx.state.next_waypoint = 0\n",
    "control.setNewWaypoint(0)\n",
    "\n",
    "path_length = len(ctx.state.path) - 1\n",
    "while ctx.state.next_waypoint_index < path_length:\n",
    "    # Get the desired control (#1)\n",
    "    (arrived, vLC, vRC) = control.controlPosition()\n",
    "\n",
    "    # Plot current state\n",
    "    clear_output(wait=True)\n",
    "    plot_map(ctx, \"Path-finding map\")\n",
    "    \n",
    "    if arrived:\n",
    "        # load the next waypoint to aim if arrived to the previous one\n",
    "        control.setNewWaypoint(1)\n",
    "    \n",
    "    # Send the motor speed to the filtering module as a Thymio event (#2)\n",
    "    filtering.process_event({\"motor.left.speed\": [int(vLC)], \"motor.right.speed\": [int(vRC)]})\n",
    "    await sleep(0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1672905",
   "metadata": {},
   "source": [
    "We need to introduce the Local Navigation block to understand completely the `controlWithDistance()` function. The full explaination can be found in the [local navigation section](#toc1_8_).\n",
    "\n",
    "Motion control receives the distance array which contains how far from the obstacle the Thymio is.\n",
    "\n",
    "Distance control is very simple:\n",
    "\n",
    "1. If no obstacles: Thymio will try to go forward and turn a bit to the right trying to find wall to follow on his left-hand side\n",
    "2. If an obstacle is close enough: \n",
    "    1. If the Thymio is really close to the obstalce, it will move backward\n",
    "    2. It will move to the right trying to avoid the obstacle\n",
    "\n",
    "Depending if the triggered sensor is frontal or from the sides, the reaction will be similar but more or less accentuated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792dc48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.local_navigation import LocalNavigation\n",
    "\n",
    "local_nav = LocalNavigation(ctx, control)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb47fa53-e17f-41a0-905c-43e49acca493",
   "metadata": {},
   "outputs": [],
   "source": [
    "local_nav.process_event(variables={\"prox.horizontal\": [2800,3000,3300,3000,3400,2000,2000]}) #simulate the sensors \n",
    "print(\"Distance array: \", ctx.state.relative_distances[:-2])\n",
    "(arrived, vLC, vRC) = control.controlWithDistance()\n",
    "print({ \"vLC\": vLC, \"vRC\": vRC })\n",
    "print(\"The Thymio is going foward and turning a bit to the right to try figure out where the wall is\\n\")\n",
    "\n",
    "\n",
    "local_nav.process_event(variables={\"prox.horizontal\": [2800,3000,4500,3000,3400,2000,2000]}) #simulate the sensors \n",
    "print(\"Distance array: \", ctx.state.relative_distances[:-2])\n",
    "(arrived, vLC2, vRC2) = control.controlWithDistance()\n",
    "print({ \"vLC\": vLC2, \"vRC\": vRC2 })\n",
    "print(\"The Thymio is going backward and turning to the left to avoid the wall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54757940",
   "metadata": {},
   "source": [
    "In our program, we are calling the funtion `update_motor_control()`, this function chooses the correct control defined in the Local navigation block. The function will also update the targeted waypoint if the Thymio arrives to the waypoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31bece6a",
   "metadata": {},
   "source": [
    "## <a id='toc1_8_'></a>[Local navigation](#toc0_)\n",
    "\n",
    "_Source: [local_navigation.py](./app/local_navigation.py)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82911935",
   "metadata": {},
   "outputs": [],
   "source": [
    "reset(ctx)\n",
    "\n",
    "from app.local_navigation import LocalNavigation\n",
    "\n",
    "local_nav = LocalNavigation(ctx, control)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51e7bc7",
   "metadata": {},
   "source": [
    "The Local Navigation block has the responsability to trigger the control by waypoint or the control with distance state depending if the Thymio senses an obstacle close to his sensors. It communicates the type of control through the state variable : `reactive_control`. \n",
    "\n",
    "This block can also detect and locate obstacle positions with the function: `getWallRelative()`.\n",
    "\n",
    "This function does not exist in the project but for the report it is very useful because we can simulate a measure of the Thymio's sensors and update the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658b5ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulateSensors(sensorsValues):\n",
    "    # simulate a Thymio proximity event\n",
    "    local_nav.process_event({\"prox.horizontal\": sensorsValues})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec486f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulateSensors([4500,3000,3300,4000,3400,2000,2000])\n",
    "print(ctx.state.relative_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8140c3",
   "metadata": {},
   "source": [
    "The function `getWallRelative()` returns the relative position of the sensed walls (relative because it depends on the position and orientation of the Thymio). This function could be useful if in the future we want to implement a scan of obstacles with the Thymio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf51a047",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulateSensors([3000,3000,3300,4000,3400,2000,2000])\n",
    "relative_obstacle_position = local_nav.getWallRelative()\n",
    "print(relative_obstacle_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2498bb",
   "metadata": {},
   "source": [
    "Now we enter the main part of the block: the type of control trigger depending on the sensors values.\n",
    "The function `should_freestyle()` defines if the Thymio should enter the reactive control mode and updates the state. It will be read by the Motion control block for it to apply the correct control. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8315a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx.state.reactive_control = False\n",
    "\n",
    "simulateSensors([3000,3000,3000,3000,3400,2000,2000]) #Reading of sensors far away from an obstacle\n",
    "print(\"relative distances : \", ctx.state.relative_distances[:-2])\n",
    "local_nav.should_freestyle()\n",
    "print(\"should enter local navigation ? : \", ctx.state.reactive_control)\n",
    "\n",
    "\n",
    "simulateSensors([3000,4500,4500,3000,3400,2000,2000])#Reading of sensors close to an obstacle, the front one at 4500\n",
    "print(\"relative distances : \" ,ctx.state.relative_distances[:-2])\n",
    "local_nav.should_freestyle()\n",
    "print(\"should enter local navigation ? : \", ctx.state.reactive_control)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1559fc7a",
   "metadata": {},
   "source": [
    "## <a id='toc1_9_'></a>[Putting it all together](#toc0_)\n",
    "\n",
    "Now that we have covered all the different modules of our application, we can put them together to run the whole thing.\n",
    "\n",
    "There are two ways to run the application.\n",
    "\n",
    "### <a id='toc1_9_1_'></a>[Running from the terminal](#toc0_)\n",
    "\n",
    "This is the **recommended** approach, it's more robust and allows you to see better application logs, stop the application with Ctrl+C and also use keyboard input using stdin.\n",
    "\n",
    "\n",
    "```powershell\n",
    "$ python -m app\n",
    "```\n",
    "\n",
    "\n",
    "### <a id='toc1_9_2_'></a>[Running from Jupyter](#toc0_)\n",
    "\n",
    "On modern versions of IPython (7+), it's possible to run the application from a Jupyter notebook using the top-level await syntax. From our experiments, the coloured terminal logging, GUI calibration window and keyboard input should all work.\n",
    "\n",
    "Note that running the following cell will block the notebook kernel until the cell is interrupted (the future is cancelled), the kernal is restarted or the Web UI is used to terminal the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e9b578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from asyncio import CancelledError\n",
    "from app.__main__ import init\n",
    "\n",
    "\n",
    "try:\n",
    "    await init()\n",
    "\n",
    "except CancelledError:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b97a24aec6f8def2674d93ea2437cb3728843830cb7d5b7fbd0461f275e8bc7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
