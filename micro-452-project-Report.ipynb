{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d38ad9a1",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Mobile Robotics Project Report](#toc0_)\n",
    "\n",
    "---\n",
    "Marcus Cemes, Pable Paller, Adrien Pannatier,  Carolina Rodrigues Fidalgo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a741a9",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"border:1px solid black; padding:20px 20px;text-align: justify;text-justify: inter-word\">\n",
    "    This notebook describes the different parts of our project for the course micro-452: Basics of mobile robotics.<br/>\n",
    "    In the following cells we will describe how the modules work and give a structure to run the project.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91923f5",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Mobile Robotics Project Report](#toc1_)    \n",
    "  - [Run requirements](#toc1_1_)    \n",
    "  - [Add the modules to path](#toc1_2_)    \n",
    "  - [Vision](#toc1_3_)    \n",
    "  - [Filtering](#toc1_4_)    \n",
    "    - [Prediction](#toc1_4_1_)    \n",
    "    - [Update](#toc1_4_2_)    \n",
    "  - [Global navigation](#toc1_5_)    \n",
    "  - [Big Brain](#toc1_6_)    \n",
    "  - [Motion control](#toc1_7_)    \n",
    "  - [Local navigation](#toc1_8_)    \n",
    "  - [Running the program as a whole](#toc1_9_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "816f8151",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Run requirements](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ed8216",
   "metadata": {},
   "source": [
    "All instructions to run the project are to be found in the [readme](README.md)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4456d199",
   "metadata": {},
   "source": [
    "## <a id='toc1_2_'></a>[Add the modules to path](#toc0_)\n",
    "\n",
    "The following trickery adds our Python module to the Jupyter environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92df5d13-08df-49fe-90dd-3e0485a0fead",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sys import path\n",
    "path.append(\"./app\")\n",
    "path.append(\"./report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e888bff-0b12-4694-a4b3-0a9b3ec5024a",
   "metadata": {},
   "source": [
    "Let's also import some useful libraries for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a95436-6c5a-4c20-a856-f09d96e45597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ipywidgets import interact, interactive, fixed, interact_manual\n",
    "import ipywidgets as widgets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa8a325-c975-4893-8385-2b0f95ebf97e",
   "metadata": {},
   "source": [
    "One of the main structures of our project is the context. Created to be shared between the modules, the context holds the following data:<br/>\n",
    "\n",
    "<span style=\"text-decoration:underline;font-weight:bold;\">[Context Class](app/context.py)</span>\n",
    "- <strong>node</strong>: node of the main thymio\n",
    "- <strong>node_top</strong>: node of the top thymio\n",
    "- <strong>pool</strong>: necessary part to allow multiple threading in python\n",
    "- <strong>state</strong>: state containing all the necessary elements to describe the scene\n",
    "- <strong>scene_update</strong>: signaling of scene update from vision\n",
    "- <strong>pose_update</strong>: signaling position update from filtering\n",
    "- <strong>debug_update</strong>: boolean to allow debugging vision display in the ui"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "904c0a34",
   "metadata": {},
   "source": [
    "Let us create this context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288a034d-dd0d-45b4-a5e7-3015ca4f4f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.config import *\n",
    "from app.context import Context\n",
    "from app.state import State\n",
    "from report.mock_pool import MockPool\n",
    "import report.map\n",
    "import report.report_functions \n",
    "\n",
    "ctx = Context(node=None, node_top=None, pool=MockPool(), state=State())\n",
    "\n",
    "ctx.state.physical_size = PHYSICAL_SIZE_CM\n",
    "ctx.state.subdivisions = SUBDIVISIONS"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb5a5f29",
   "metadata": {},
   "source": [
    "The [State Class](app/state.py) holds the necessary information on the thymio to allow the correct progression of the robot.</br>\n",
    "State also contains the elements needed by the ui interface to display the scene. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f7eb1ed",
   "metadata": {},
   "source": [
    "In order to visualise the state of the map without the use of the Web UI, we've written a `plot_map(ctx)` function that uses the matplotlib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960784ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "report.map.plot_map(ctx, \"map of the scene\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6e670b5",
   "metadata": {},
   "source": [
    "We can now import modules and show how they work!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "663d573b",
   "metadata": {},
   "source": [
    "___\n",
    "## <a id='toc1_3_'></a>[Vision](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffaf769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.vision import Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455561c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision = Vision(ctx, external=False, live=False, image_path=\"assets/test_frame_01.jpg\")\n",
    "report.map.reset(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc89670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision.calibrate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a15ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = vision.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2938f9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_pose = (obs.back[0], obs.back[1], report.report_functions.angle(obs.back, obs.front))\n",
    "print(vision_pose)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "939b1fc9",
   "metadata": {},
   "source": [
    "display the vision retranscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b79a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx.state.position = (obs.back[0], obs.back[1])\n",
    "ctx.state.orientation = report.report_functions.angle(obs.back, obs.front)\n",
    "ctx.state.obstacles = obs.obstacles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed55e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "report.map.plot_map(ctx, \"vision retranscription\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b89e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "report.map.show_picture(\"assets/test_frame_01.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a0a234d",
   "metadata": {},
   "outputs": [],
   "source": [
    "report.map.plot_map(ctx, \"Vision Map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f528a8e8",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Filtering](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9184e358",
   "metadata": {},
   "source": [
    "Another important module of our project is filtering. Filtering is taking the information given by sensors and the camera to be able to guess where the thymio is on the map and where it's going.</br>\n",
    "For this project we decieded to use an [Extanded Kalman Filter](app/EXF.py) and an intermediate module called [filtering](app/filtering.py) to comunicate with the main module.</br>\n",
    "The EKF was chosen for its capability of predicting a non-linear evolution of the state and also to be able to dissociate information coming from the motors and information given by the camera.</br>\n",
    "For this purpose, two functions where used: [predict](#toc1_4_1_) and [update](#toc1_4_2_). We will come back to these parts in the subsections below.\n",
    "___\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8262ce5f",
   "metadata": {},
   "source": [
    "first let's import the module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a783d006-43b1-41db-9b4d-cea07e80d151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.filtering import Filtering\n",
    "from asyncio import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79198013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to reset this section\n",
    "#creation of the filter\n",
    "filtering = Filtering(ctx)\n",
    "report.map.reset(ctx)\n",
    "report.report_functions.start_movement_simulation(filtering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df9ac02",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_1_'></a>[Prediction](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0727adbe",
   "metadata": {},
   "source": [
    "A specificity of our project is the time delay between camera and motor sensors. Since the motor sensors response is faster than the one from vision, what we do is predict the position of the robot with the motor sensors at a greater frequency than the update with vision information. The time interval between each prediction is dynamic and adapts to correctly predict each time the motor sensors send variables.\n",
    "\n",
    "The next position and orientation are then predicted.\n",
    "\n",
    "Let us run it to see how it works\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eea28e-738a-4ad9-9c3f-c5c355245e14",
   "metadata": {},
   "source": [
    "Let's take a look at our current position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17073ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "report.report_functions.print_pose(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6efbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "report.map.plot_map(ctx, \"Filtering Map\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1e8ca65-a9b1-4899-a710-2d010667779a",
   "metadata": {},
   "source": [
    "To execute a filtering step let's make the Thymio move forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7764f84c-156b-4b85-ae62-e9dec4ff37e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering.process_event({\"motor.left.speed\": [100], \"motor.right.speed\": [100]})\n",
    "report.report_functions.stop_movement_simulation(filtering)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37ca19d6-98c8-48a9-9395-84acfb11daf6",
   "metadata": {},
   "source": [
    "The state should now be updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c973de18-b444-4774-ac4e-f7f84117f772",
   "metadata": {},
   "outputs": [],
   "source": [
    "report.report_functions.print_pose(ctx)\n",
    "report.map.plot_map(ctx, \"Filtering Map\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f19687a",
   "metadata": {},
   "source": [
    "Let's do it again, turning the robot this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5ccfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering.process_event({\"motor.left.speed\": [50], \"motor.right.speed\": [-50]})\n",
    "report.report_functions.stop_movement_simulation(filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98537fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "report.report_functions.print_pose(ctx)\n",
    "report.map.plot_map(ctx, \"Filtering Map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d77519",
   "metadata": {},
   "source": [
    "we see that filtering is correctly predicting the progression of the simulated robot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccabfc0f",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_2_'></a>[Update](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed29ff62",
   "metadata": {},
   "source": [
    "To reduce the error caused by motors, vision must be used as much as possible. If vision is lost for a while, the update step brings back the Thymio to it's correct position taking into consideration the computed prediction since it's disapearing.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5211d6",
   "metadata": {},
   "source": [
    "now to display the update function, we use the image and position of the robot given by the [Vision module](#toc1_3_)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2208520",
   "metadata": {},
   "source": [
    "The position after our prediction step is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155ea781",
   "metadata": {},
   "outputs": [],
   "source": [
    "report.report_functions.print_pose(ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e43b13",
   "metadata": {},
   "source": [
    "now we update the position with the one given by vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea0726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_position = vision #get image from vision\n",
    "filtering.update(vision_pose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb21370f",
   "metadata": {},
   "source": [
    "after the vision update, the new position is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b098a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "report.report_functions.print_pose(ctx)\n",
    "report.map.plot_map(ctx, \"Filtering Map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edd6cf89",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[Global navigation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fa2c9a-b7e0-4a09-8205-5c5f14549425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import app.global_navigation\n",
    "\n",
    "global_nav = app.global_navigation.GlobalNavigation(ctx)\n",
    "\n",
    "# Reset the state related to path finding.\n",
    "# You can rerun this cell to reset this section.\n",
    "report.map.reset(ctx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fa05483-f170-4b17-aec4-a3f15709782f",
   "metadata": {},
   "source": [
    "At the moment our map is completely empty. In order to invoke our pathfinding algorithm, we need to set the <font color='red'>Thymio's position</font>  and a desired <font color='cyan'>end point</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031257b3-5db9-4211-a549-bbf6ca99c3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the state\n",
    "ctx.state.position = (10, 10)\n",
    "ctx.state.end = (90, 110)\n",
    "\n",
    "# Recompute the path\n",
    "await global_nav._recompute_path()\n",
    "\n",
    "# Plot the result\n",
    "report.map.plot_map(ctx, \"Path-finding map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1862c56-0290-462f-bb49-6cf72c3c6be1",
   "metadata": {},
   "source": [
    "**🎉 We have a path!**\n",
    "\n",
    "Let's not celebrate yet, we can make this much more interesting. Our pathfinding uses an implementation of the [Dijkstra's algorithm](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm). While this may not be the most fastest or most efficient algorithm, it _does_ guarantee optimal results for a given graph.\n",
    "\n",
    "Our node graph is a grid of equaly spaced cells, where edges are made between directly adjacent cells and diagonal cells, with a cost of 1 and $\\sqrt{2}$, respectively. This allows our robot to also travel diagonally and avoid grid-like movement.\n",
    "\n",
    "<div align=\"center\" style=\"padding: 1rem 0;\">\n",
    "    <figure>\n",
    "        <img src=\"assets/report/dijkstra-nodes.png\" style=\"width: 8rem;\" />\n",
    "        <figcaption style=\"font-style: italic;\">Edges for a graph node to its adjacent nodes.</figcaption>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "Ideally, we would like all nodes to be connected to every other node, given that there is a line-of-sight between them, however this comes with a huge computational burden. This simple implementation (mixed with our path optimisation algorithm covered later) provides adequate results in roughly 50 ms for 64 subdivisions.\n",
    "\n",
    "Time for some obstacles! Obstacles come directly from the vision module as a matrix of 8-bit integers, where any value other an 0 represent an occupied cell. The size of the matrix coincides with the node graph, an occupied cell will mark a node as unvisitable.\n",
    "\n",
    "**Note:** While we could have also used a matrix of booleans, integers are slightly easier to work with and compose with other functions such as convolutions. Additionally, it has a smaller JSON serialisation footprint for sending over WebSockets to the Web UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce68b7b-0da6-4a1f-9831-4164dcae34fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the matrix\n",
    "ctx.state.obstacles[:,:] = 0\n",
    "\n",
    "# Add two obstacles\n",
    "ctx.state.obstacles[30:45, 40:55] = 1\n",
    "ctx.state.obstacles[20:30, 5: 20] = 1\n",
    "\n",
    "# Recompute the path\n",
    "await global_nav._recompute_path()\n",
    "\n",
    "# Plot the result\n",
    "report.map.plot_map(ctx, \"Path-finding map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ad200-eee1-4a3d-9148-760c033a7b21",
   "metadata": {},
   "source": [
    "The dark-blue region represents the obstacles that we set. The cyan regions are the obstacle boundaries that are generated using a convolution of a circular kernel matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f01aae-80a6-4476-b9dd-287c0007d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = global_nav._safety_margin_kernel()\n",
    "\n",
    "report.map.plot_image(kernel, \"Obstacle boundary convolution kernel\", colourbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916d9a38-5988-44e6-a868-db8a058f38e0",
   "metadata": {},
   "source": [
    "The boundary is equaly treated as unvisitable by the pathfinding algorith. This ensures that the Thymio does not hit any objects with its wheels as it's navigating the path. It a gap is too small, the Thymio will not try to fit through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc68a70-6455-42bf-a33f-f6299aeeb3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the matrix\n",
    "ctx.state.obstacles[:,:] = 0\n",
    "\n",
    "# Add two obstacles with a gap\n",
    "ctx.state.obstacles[31:33, 15:29] = 1\n",
    "ctx.state.obstacles[31:33, 42:50] = 1\n",
    "\n",
    "# Recompute the path\n",
    "await global_nav._recompute_path()\n",
    "\n",
    "# Plot the result\n",
    "report.map.plot_map(ctx, \"Path-finding map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aca0e1b-d678-4d9b-9809-5d9d30964ef6",
   "metadata": {},
   "source": [
    "### Path optimisation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8a7fc5-bff9-43ee-a8fd-d9624d349191",
   "metadata": {},
   "source": [
    "Our chosen node graph constraints our robots movements more or less to a grid, with the ability of also being able to move diagonally. This creates a lot of path waypoints, but also creates less natural-like movement.\n",
    "\n",
    "To combat this, we have an additional post-pathfinding step to iteratively try to reduce the path to as few waypoints as necessary that have free line of sight between them. This does not find the most optimal solution, but the results are more than adequate.\n",
    "\n",
    "The algorithm goes as follows:\n",
    "\n",
    "1. For $i = 2,...,M-1$, where $M$ is the number of waypoints\n",
    "2. Check whether there is a free line-of-sight between waypoints $w_{i-1}$ and $w_{i+1}$\n",
    "3. If yes, remove the waypoint $w_i$, otherwise consider the next waypoint $w_{i+1}$\n",
    "\n",
    "In order to calculate free line-of-sight, it's necessary to enumerate all map cells that the segment between $w_{i-1}$ and $w_{i+1}$ travels through to check their occupation status. This is done using an algorithm such as [Bresenham's line algorithm](https://en.wikipedia.org/wiki/Bresenham%27s_line_algorithm). We found an improved version that also uses integer-only math [here](https://playtechs.blogspot.com/2007/03/raytracing-on-grid.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bb9c40-6fda-40c1-8730-e0a7f0f7c175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.path_finding.path_optimiser import raytrace\n",
    "\n",
    "p1 = (1, 2)\n",
    "p2 = (8, 6)\n",
    "\n",
    "matrix = np.zeros((10, 10))\n",
    "\n",
    "for (i, j) in raytrace(p1, p2):\n",
    "    matrix[j][i] = 1\n",
    "\n",
    "report.map.plot_raytrace(matrix, p1, p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e684f7c-1e87-4298-ad7e-3fd33f89b28b",
   "metadata": {},
   "source": [
    "Let's put this into practice using the path that we found previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a91d176-ac3b-42e9-88c7-175818e8c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.path_finding.path_optimiser import PathOptimiser\n",
    "\n",
    "map = report.map.create_map(ctx)\n",
    "\n",
    "optimiser = PathOptimiser(map)\n",
    "\n",
    "# The output of the pathfinding algorithm is in physical space [cm],\n",
    "# it needs to be converted back into map indicies for this demonstration.\n",
    "# Usually the path optimisation is done as an intermediate step by the\n",
    "# path finding algorithm, before the conversion to physical space.\n",
    "path = report.map.path_to_coords(ctx.state.path)\n",
    "\n",
    "# The optimiser mutates the input list directly\n",
    "optimised_path = optimiser.optimise(path.copy())\n",
    "\n",
    "report.map.plot_path_optimisation(path, optimised_path, map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1ad1fd-2f4a-4207-b794-703dd6b9daa9",
   "metadata": {},
   "source": [
    "Path optimisation can be toggled dynamically by setting the `ctx.state.optimise` boolean flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca233e2-1175-489f-96dc-6dee946cdac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable path optimisation\n",
    "ctx.state.optimise = True\n",
    "\n",
    "# Reset the matrix\n",
    "ctx.state.obstacles[:,:] = 0\n",
    "\n",
    "# Add two obstacles\n",
    "ctx.state.obstacles[30:45, 40:55] = 1\n",
    "ctx.state.obstacles[20:30, 5: 20] = 1\n",
    "\n",
    "await global_nav._recompute_path()\n",
    "\n",
    "report.map.plot_map(ctx, \"Path-finding map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8eab67c",
   "metadata": {},
   "source": [
    "## <a id='toc1_6_'></a>[Big Brain](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfd658af",
   "metadata": {},
   "source": [
    "## <a id='toc1_7_'></a>[Motion control](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8976982e",
   "metadata": {},
   "source": [
    "## <a id='toc1_8_'></a>[Local navigation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37df7b0a",
   "metadata": {},
   "source": [
    "## <a id='toc1_9_'></a>[Running the program as a whole](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf0736b7",
   "metadata": {},
   "source": [
    "now that we went through all the main modules we can run the app containing the project"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a20b3a6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m app"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c663c1a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "c261aea317cc0286b3b3261fbba9abdec21eaa57589985bb7a274bf54d6cc0a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
