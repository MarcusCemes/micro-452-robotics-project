{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d38ad9a1",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Mobile Robotics Project Report](#toc0_)\n",
    "\n",
    "---\n",
    "Marcus Cemes, Pable Paller, Adrien Pannatier,  Carolina Rodrigues Fidalgo"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "65a741a9",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"border:1px solid black; padding:20px 20px;text-align: justify;text-justify: inter-word\">\n",
    "    This notebook describes the different parts of our project for the course micro-452: Basics of mobile robotics.<br/>\n",
    "    In the following cells we will describe how the modules work and give a structure to run the project.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f91923f5",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>\n",
    "\n",
    "- [Mobile Robotics Project Report](#toc1_)\n",
    "  - [Run requirements](#toc1_1_)\n",
    "  - [Add the modules to path](#toc1_2_)\n",
    "  - [Vision](#toc1_3_)\n",
    "  - [Filtering](#toc1_4_)\n",
    "    - [Prediction](#toc1_4_1_)\n",
    "    - [Update](#toc1_4_2_)\n",
    "  - [Global navigation](#toc1_5_)\n",
    "  - [Big Brain](#toc1_6_)\n",
    "  - [Motion control](#toc1_7_)\n",
    "  - [Local navigation](#toc1_8_)\n",
    "  - [Putting it all together](#toc1_9_)\n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "816f8151",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Prerequisites](#toc0_)\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Tip:</b> In the latest Python/Jupyter version, it should no longer be necsesary to add our application module to the path. If you experience any import errors, convert the following block to code and run it before anything else.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c264085-9b4e-4ab4-b916-cdce9283d090",
   "metadata": {},
   "source": [
    "from sys import path\n",
    "path.append(\"./app\")\n",
    "path.append(\"./report\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1d1ebe28-e5a2-432d-9f98-9125c918c382",
   "metadata": {},
   "source": [
    "This project has been tested on **Python 3.10** and **JupyterLab 3.5**, allowing for features such as:\n",
    "\n",
    "- Python's new `match` statement\n",
    "- Top-level async/await\n",
    "- Integrated event loop in the notebook\n",
    "\n",
    "To make sure that your environment is supported, try running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c431f131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from report.utils import run_checks\n",
    "\n",
    "run_checks()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "58b29f90",
   "metadata": {},
   "source": [
    "If you have any missing requirements, convert the following into a code block and run it (a [venv](https://docs.python.org/3/library/venv.html) is recommended)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "607d87d6-0b91-451f-8d29-d70634d9ad8d",
   "metadata": {},
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ed8216",
   "metadata": {},
   "source": [
    "Additional installation instructions can be found in the project's [README.md](README.md).\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0a0b833a-1329-4eb6-909c-f9fcc88e4802",
   "metadata": {},
   "source": [
    "### Useful imports\n",
    "\n",
    "We use these libraries throughout the notebook, run this cell to make them available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a95436-6c5a-4c20-a856-f09d96e45597",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "baa8a325-c975-4893-8385-2b0f95ebf97e",
   "metadata": {},
   "source": [
    "One of the main structures of our project is the context. Created to be shared between the modules, the context holds the following data:<br/>\n",
    "\n",
    "<span style=\"text-decoration:underline;font-weight:bold;\">[Context Class](app/context.py)</span>\n",
    "- <strong>node</strong>: node of the main thymio\n",
    "- <strong>node_top</strong>: node of the top thymio\n",
    "- <strong>pool</strong>: necessary part to allow multiple threading in python\n",
    "- <strong>state</strong>: state containing all the necessary elements to describe the scene\n",
    "- <strong>scene_update</strong>: signaling of scene update from vision\n",
    "- <strong>pose_update</strong>: signaling position update from filtering\n",
    "- <strong>debug_update</strong>: boolean to allow debugging vision display in the ui"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "904c0a34",
   "metadata": {},
   "source": [
    "Let us create this context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288a034d-dd0d-45b4-a5e7-3015ca4f4f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.config import *\n",
    "from app.context import Context\n",
    "from app.state import State\n",
    "from report.mock_pool import MockPool\n",
    "import report.map\n",
    "import report.report_functions \n",
    "\n",
    "ctx = Context(node=None, node_top=None, pool=MockPool(), state=State())\n",
    "\n",
    "ctx.state.physical_size = PHYSICAL_SIZE_CM\n",
    "ctx.state.subdivisions = SUBDIVISIONS"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "eb5a5f29",
   "metadata": {},
   "source": [
    "The [State Class](app/state.py) holds the necessary information on the thymio to allow the correct progression of the robot.</br>\n",
    "State also contains the elements needed by the ui interface to display the scene. "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3f7eb1ed",
   "metadata": {},
   "source": [
    "In order to visualise the state of the map without the use of the Web UI, we've written a `plot_map(ctx)` function that uses the matplotlib library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960784ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "report.map.plot_map(ctx, \"map of the scene\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a6e670b5",
   "metadata": {},
   "source": [
    "We can now import modules and show how they work!"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "663d573b",
   "metadata": {},
   "source": [
    "___\n",
    "## <a id='toc1_3_'></a>[Vision](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffaf769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.vision import Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455561c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision = Vision(ctx, external=False, live=False, image_path=\"assets/test_frame_01.jpg\")\n",
    "report.map.reset(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc89670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision.calibrate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a15ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = vision.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2938f9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_pose = (obs.back[0], obs.back[1], report.report_functions.angle(obs.back, obs.front))\n",
    "print(vision_pose)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "939b1fc9",
   "metadata": {},
   "source": [
    "display the vision retranscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b79a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx.state.position = (obs.back[0], obs.back[1])\n",
    "ctx.state.orientation = report.report_functions.angle(obs.back, obs.front)\n",
    "ctx.state.obstacles = obs.obstacles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed55e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "report.map.plot_map(ctx, \"vision retranscription\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b89e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "report.map.show_picture(\"assets/test_frame_01.jpg\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f528a8e8",
   "metadata": {},
   "source": [
    "## <a id='toc1_4_'></a>[Filtering](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9184e358",
   "metadata": {},
   "source": [
    "Another important module of our project is filtering. Filtering is taking the information given by sensors and the camera to be able to guess where the thymio is on the map and where it's going.</br>\n",
    "For this project we decieded to use an [Extanded Kalman Filter](app/EXF.py) and an intermediate module called [filtering](app/filtering.py) to comunicate with the main module.</br>\n",
    "The EKF was chosen for its capability of predicting a non-linear evolution of the state and also to be able to dissociate information coming from the motors and information given by the camera.</br>\n",
    "For this purpose, two functions where used: [predict](#toc1_4_1_) and [update](#toc1_4_2_). We will come back to these parts in the subsections below.\n",
    "___\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8262ce5f",
   "metadata": {},
   "source": [
    "first let's import the module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a783d006-43b1-41db-9b4d-cea07e80d151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.filtering import Filtering\n",
    "from asyncio import sleep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79198013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to reset this section\n",
    "#creation of the filter\n",
    "filtering = Filtering(ctx)\n",
    "report.map.reset(ctx)\n",
    "report.report_functions.start_movement_simulation(filtering)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8df9ac02",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_1_'></a>[Prediction](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0727adbe",
   "metadata": {},
   "source": [
    "A specificity of our project is the time delay between camera and motor sensors. Since the motor sensors response is faster than the one from vision, what we do is predict the position of the robot with the motor sensors at a greater frequency than the update with vision information. The time interval between each prediction is dynamic and adapts to correctly predict each time the motor sensors send variables.\n",
    "\n",
    "The next position and orientation are then predicted.\n",
    "\n",
    "Let us run it to see how it works\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64eea28e-738a-4ad9-9c3f-c5c355245e14",
   "metadata": {},
   "source": [
    "Let's take a look at our current position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17073ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "report.report_functions.print_pose(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6efbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "report.map.plot_map(ctx, \"Filtering Map\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b1e8ca65-a9b1-4899-a710-2d010667779a",
   "metadata": {},
   "source": [
    "To execute a filtering step let's make the Thymio move forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7764f84c-156b-4b85-ae62-e9dec4ff37e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering.process_event({\"motor.left.speed\": [70], \"motor.right.speed\": [70]})\n",
    "report.report_functions.stop_movement_simulation(filtering)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37ca19d6-98c8-48a9-9395-84acfb11daf6",
   "metadata": {},
   "source": [
    "The state should now be updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c973de18-b444-4774-ac4e-f7f84117f772",
   "metadata": {},
   "outputs": [],
   "source": [
    "report.report_functions.print_pose(ctx)\n",
    "report.map.plot_map(ctx, \"Filtering Map\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4f19687a",
   "metadata": {},
   "source": [
    "Let's do it again, turning the robot this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5ccfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering.process_event({\"motor.left.speed\": [50], \"motor.right.speed\": [-50]})\n",
    "report.report_functions.stop_movement_simulation(filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98537fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "report.report_functions.print_pose(ctx)\n",
    "report.map.plot_map(ctx, \"Filtering Map\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "64d77519",
   "metadata": {},
   "source": [
    "we see that filtering is correctly predicting the progression of the simulated robot"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ccabfc0f",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_2_'></a>[Update](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ed29ff62",
   "metadata": {},
   "source": [
    "To reduce the error caused by motors, vision must be used as much as possible. If vision is lost for a while, the update step brings back the Thymio to it's correct position taking into consideration the computed prediction since it's disapearing.\n",
    "___"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "0c5211d6",
   "metadata": {},
   "source": [
    "now to display the update function, we use the image and position of the robot given by the [Vision module](#toc1_3_)   "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a2208520",
   "metadata": {},
   "source": [
    "The position after our prediction step is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155ea781",
   "metadata": {},
   "outputs": [],
   "source": [
    "report.report_functions.print_pose(ctx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "12e43b13",
   "metadata": {},
   "source": [
    "now we update the position with the one given by vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea0726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_position = vision #get image from vision\n",
    "filtering.update(vision_pose)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb21370f",
   "metadata": {},
   "source": [
    "after the vision update, the new position is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b098a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "report.report_functions.print_pose(ctx)\n",
    "report.map.plot_map(ctx, \"Filtering Map\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "edd6cf89",
   "metadata": {},
   "source": [
    "## <a id='toc1_5_'></a>[Global navigation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fa2c9a-b7e0-4a09-8205-5c5f14549425",
   "metadata": {},
   "outputs": [],
   "source": [
    "import app.global_navigation\n",
    "\n",
    "global_nav = app.global_navigation.GlobalNavigation(ctx)\n",
    "\n",
    "# Reset the state related to path finding.\n",
    "# You can rerun this cell to reset this section.\n",
    "report.map.reset(ctx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4fa05483-f170-4b17-aec4-a3f15709782f",
   "metadata": {},
   "source": [
    "At the moment our map is completely empty. In order to invoke our pathfinding algorithm, we need to set the <font color='red'>Thymio's position</font>  and a desired <font color='cyan'>end point</font>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031257b3-5db9-4211-a549-bbf6ca99c3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update the state\n",
    "ctx.state.position = (10, 10)\n",
    "ctx.state.end = (90, 110)\n",
    "\n",
    "# Recompute the path\n",
    "await global_nav._recompute_path()\n",
    "\n",
    "# Plot the result\n",
    "report.map.plot_map(ctx, \"Path-finding map\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a1862c56-0290-462f-bb49-6cf72c3c6be1",
   "metadata": {},
   "source": [
    "**ðŸŽ‰ We have a path!**\n",
    "\n",
    "Let's not celebrate yet, we can make this much more interesting. Our pathfinding uses an implementation of the [Dijkstra's algorithm](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm). While this may not be the most fastest or most efficient algorithm, it _does_ guarantee optimal results for a given graph.\n",
    "\n",
    "Our node graph is a grid of equaly spaced cells, where edges are made between directly adjacent cells and diagonal cells, with a cost of 1 and $\\sqrt{2}$, respectively. This allows our robot to also travel diagonally and avoid grid-like movement.\n",
    "\n",
    "<div align=\"center\" style=\"padding: 1rem 0;\">\n",
    "    <figure>\n",
    "        <img src=\"assets/report/dijkstra-nodes.png\" style=\"width: 8rem;\" />\n",
    "        <figcaption style=\"font-style: italic;\">Edges for a graph node to its adjacent nodes.</figcaption>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "Ideally, we would like all nodes to be connected to every other node, given that there is a line-of-sight between them, however this comes with a huge computational burden. This simple implementation (mixed with our path optimisation algorithm covered later) provides adequate results in roughly 50 ms for 64 subdivisions.\n",
    "\n",
    "Time for some obstacles! Obstacles come directly from the vision module as a matrix of 8-bit integers, where any value other an 0 represent an occupied cell. The size of the matrix coincides with the node graph, an occupied cell will mark a node as unvisitable.\n",
    "\n",
    "**Note:** While we could have also used a matrix of booleans, integers are slightly easier to work with and compose with other functions such as convolutions. Additionally, it has a smaller JSON serialisation footprint for sending over WebSockets to the Web UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce68b7b-0da6-4a1f-9831-4164dcae34fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the matrix\n",
    "ctx.state.obstacles[:,:] = 0\n",
    "\n",
    "# Add two obstacles\n",
    "ctx.state.obstacles[30:45, 40:55] = 1\n",
    "ctx.state.obstacles[20:30, 5: 20] = 1\n",
    "\n",
    "# Recompute the path\n",
    "await global_nav._recompute_path()\n",
    "\n",
    "# Plot the result\n",
    "report.map.plot_map(ctx, \"Path-finding map\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "a41ad200-eee1-4a3d-9148-760c033a7b21",
   "metadata": {},
   "source": [
    "The dark-blue region represents the obstacles that we set. The cyan regions are the obstacle boundaries that are generated using a convolution of a circular kernel matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f01aae-80a6-4476-b9dd-287c0007d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = global_nav._safety_margin_kernel()\n",
    "\n",
    "report.map.plot_image(kernel, \"Obstacle boundary convolution kernel\", colourbar=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "916d9a38-5988-44e6-a868-db8a058f38e0",
   "metadata": {},
   "source": [
    "The boundary is equaly treated as unvisitable by the pathfinding algorith. This ensures that the Thymio does not hit any objects with its wheels as it's navigating the path. It a gap is too small, the Thymio will not try to fit through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc68a70-6455-42bf-a33f-f6299aeeb3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the matrix\n",
    "ctx.state.obstacles[:,:] = 0\n",
    "\n",
    "# Add two obstacles with a gap\n",
    "ctx.state.obstacles[31:33, 15:29] = 1\n",
    "ctx.state.obstacles[31:33, 42:50] = 1\n",
    "\n",
    "# Recompute the path\n",
    "await global_nav._recompute_path()\n",
    "\n",
    "# Plot the result\n",
    "report.map.plot_map(ctx, \"Path-finding map\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1aca0e1b-d678-4d9b-9809-5d9d30964ef6",
   "metadata": {},
   "source": [
    "### Path optimisation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "fb8a7fc5-bff9-43ee-a8fd-d9624d349191",
   "metadata": {},
   "source": [
    "Our chosen node graph constraints our robots movements more or less to a grid, with the ability of also being able to move diagonally. This creates a lot of path waypoints, but also creates less natural-like movement.\n",
    "\n",
    "To combat this, we have an additional post-pathfinding step to iteratively try to reduce the path to as few waypoints as necessary that have free line of sight between them. This does not find the most optimal solution, but the results are more than adequate.\n",
    "\n",
    "The algorithm goes as follows:\n",
    "\n",
    "1. For $i = 2,...,M-1$, where $M$ is the number of waypoints\n",
    "2. Check whether there is a free line-of-sight between waypoints $w_{i-1}$ and $w_{i+1}$\n",
    "3. If yes, remove the waypoint $w_i$, otherwise consider the next waypoint $w_{i+1}$\n",
    "\n",
    "In order to calculate free line-of-sight, it's necessary to enumerate all map cells that the segment between $w_{i-1}$ and $w_{i+1}$ travels through to check their occupation status. This is done using an algorithm such as [Bresenham's line algorithm](https://en.wikipedia.org/wiki/Bresenham%27s_line_algorithm). We found an improved version that also uses integer-only math [here](https://playtechs.blogspot.com/2007/03/raytracing-on-grid.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bb9c40-6fda-40c1-8730-e0a7f0f7c175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.path_finding.path_optimiser import raytrace\n",
    "\n",
    "p1 = (1, 2)\n",
    "p2 = (8, 6)\n",
    "\n",
    "matrix = np.zeros((10, 10))\n",
    "\n",
    "for (i, j) in raytrace(p1, p2):\n",
    "    matrix[j][i] = 1\n",
    "\n",
    "report.map.plot_raytrace(matrix, p1, p2)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6e684f7c-1e87-4298-ad7e-3fd33f89b28b",
   "metadata": {},
   "source": [
    "Let's put this into practice using the path that we found previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a91d176-ac3b-42e9-88c7-175818e8c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.path_finding.path_optimiser import PathOptimiser\n",
    "\n",
    "map = report.map.create_map(ctx)\n",
    "\n",
    "optimiser = PathOptimiser(map)\n",
    "\n",
    "# The output of the pathfinding algorithm is in physical space [cm],\n",
    "# it needs to be converted back into map indicies for this demonstration.\n",
    "# Usually the path optimisation is done as an intermediate step by the\n",
    "# path finding algorithm, before the conversion to physical space.\n",
    "path = report.map.path_to_coords(ctx.state.path)\n",
    "\n",
    "# The optimiser mutates the input list directly\n",
    "optimised_path = optimiser.optimise(path.copy())\n",
    "\n",
    "report.map.plot_path_optimisation(path, optimised_path, map)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c1ad1fd-2f4a-4207-b794-703dd6b9daa9",
   "metadata": {},
   "source": [
    "Path optimisation can be toggled dynamically by setting the `ctx.state.optimise` boolean flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca233e2-1175-489f-96dc-6dee946cdac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable path optimisation\n",
    "ctx.state.optimise = True\n",
    "\n",
    "# Reset the matrix\n",
    "ctx.state.obstacles[:,:] = 0\n",
    "\n",
    "# Add two obstacles\n",
    "ctx.state.obstacles[30:45, 40:55] = 1\n",
    "ctx.state.obstacles[20:30, 5: 20] = 1\n",
    "\n",
    "await global_nav._recompute_path()\n",
    "\n",
    "report.map.plot_map(ctx, \"Path-finding map\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f8eab67c",
   "metadata": {},
   "source": [
    "## <a id='toc1_6_'></a>[Big Brain](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd21e289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import app.big_brain\n",
    "\n",
    "big_brain = app.big_brain.BigBrain(ctx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8744907c",
   "metadata": {},
   "source": [
    "_Source: [big_brain.py](app/big_brain.py)_\n",
    "\n",
    "The **Big Brain** module is in charge of the main application logic. It's purpose is to initialise the other modules and to orchestrate the flow of data between them.\n",
    "\n",
    "The submodules are able to run autonomously in the background of the application using Python's `with` statement and the `asyncio` library. The `__enter__` and `__exit__` methods are used to initialise and clean up the resources used by the submodules, such as long running tasks or registering and unregistering Thymio event handlers.\n",
    "\n",
    "Once the submodules are initialised, the main loop of the application is started. This loop is responsible for acquiring new data from the vision module and providing this data to the filtering and global navigation modules, which then trickle down to motion control and to the Web UI.\n",
    "\n",
    "### Significant updates\n",
    "\n",
    "To reduce the number of map updates from the vision module, we have implemented a simple thresholding function that checks whether the $L_1$ norm between the current and previous map is greater than a given threshold (i.e. a certain number of grid squares have changed). This reduces redundant pathfinding computation and also renders motion control more stable, as it is less likely to be interrupted by a sudden change in the map and path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c56636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx.state.obstacles = np.zeros((SUBDIVISIONS, SUBDIVISIONS), dtype=np.int8)\n",
    "new_map = np.zeros((SUBDIVISIONS, SUBDIVISIONS), dtype=np.int8)\n",
    "\n",
    "# Changing a signal square does not report a significant change\n",
    "new_map[1, 1] = 1\n",
    "\n",
    "big_brain.significant_change(new_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d36e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing more than the threshold of squares should report a significant change\n",
    "new_map[1:5, 1:5] = 1\n",
    "\n",
    "big_brain.significant_change(new_map, threshold=10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "30ac3db0",
   "metadata": {},
   "source": [
    "If the new map is deemed to have a significant change, the obstacle map is updated in `ctx.state` and the `ctx.scene_update` signal is triggered, which in turn triggers the global navigation module to recompute the path.\n",
    "\n",
    "Regardless of whether the map has changed or not, the new detected position is sent to the filtering module in order to update the robot's position and attempt to correct any drift.\n",
    "\n",
    "### Christmas animation\n",
    "\n",
    "Once the robot reaches the arrival point, it will start to play a celebration sequence. This is also handled by the big brain module, invoking the `ChristmasCelebration` class to play various actions, such as dropping the bauble using the second Thymio node."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cfd658af",
   "metadata": {},
   "source": [
    "## <a id='toc1_7_'></a>[Motion control](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7ecf9330",
   "metadata": {},
   "source": [
    "The block motion control has the responsability to move the thymio. He has 2 type of control, the positional control 'controlPosition()' which has the objective to go to a special set of coordinate. And the reactive control : 'controlWithDistance' which has the objective to avoid any wall and follow it by the left\n",
    "\n",
    "Motion control has also the responsability to update the waypoint he has to go if he arrives to the previous one. But also if he arrives to the last waypoint commmunicate it to the rest of the program ! \n",
    "\n",
    "1) Update the waypoint when arrived\n",
    "2) If arrived to the end of the path, change the state to let the rest of the program know "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7717523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.motion_control import MotionControl\n",
    "#creation of the filter\n",
    "control = MotionControl(ctx)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "05f1e13d",
   "metadata": {},
   "source": [
    "Let's simulate a path follow  !\n",
    "\n",
    "SETUP :\n",
    "1) We have do define a path (noramlly given by BigBrain in Global-nav)\n",
    "2) We have to define which waypoint we are aiming\n",
    "\n",
    "\n",
    "SIMULATION : \n",
    "1) We control the Thymio to go to the specified waypoint we want to go\n",
    "2) We update his position (normally given by Filtering but here let's force the update of position)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c97dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETUP\n",
    "#1)\n",
    "ctx.state.boundary_map = None\n",
    "ctx.state.obstacles = None\n",
    "report.map.reset(ctx)\n",
    "\n",
    "ctx.state.path = [[10, 0], [10, 10], [20, 10], [0,0], [0,0]] #the path we want to follow\n",
    "ctx.state.position = [0,0]\n",
    "filtering.ekf.__init__(ctx.state.position, 0) #let's initialise the Thymio position to [0,0] and an orientation of 0\n",
    "ctx.state.next_waypoint_index = 0 # we want to have the first waypoint \n",
    "ctx.state.reactive_control = False\n",
    "ctx.state.orientation = 0\n",
    "\n",
    "\n",
    "#2)\n",
    "control.setNewWaypoint(0) #load the next waypoint we want to go\n",
    "print(\"objective waypoint : \",control.waypoint) #the waypoint to go"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6beff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "\n",
    "while(ctx.state.next_waypoint_index < len(ctx.state.path)-1):\n",
    "    clear_output(wait=True)\n",
    "    report.map.plot_map(ctx, \"Path-finding map\")\n",
    "    \n",
    "    (arrived, vLC, vRC) = control.controlPosition() #1)   :    Get the control\n",
    "    if(arrived):\n",
    "        control.setNewWaypoint(1) #load the next waypoint to go if arrived to the previous one\n",
    "    \n",
    "    filtering.process_event({\"motor.left.speed\": [int(vLC)], \"motor.right.speed\": [int(vRC)]}) #    2)   force an update of position because\n",
    "    time.sleep(0.6)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d1672905",
   "metadata": {},
   "source": [
    "We need to introduce the Local Navigation block to understand completly the 'controlWithDistance' but we still gonna implement it. You can go back afterwards to undertand it :)\n",
    "\n",
    "The motion control receives the distance array which is how far from the obstacle the Thymio is.\n",
    "\n",
    "\n",
    "The distance control is very simple:\n",
    "\n",
    "1) If no obstacles : He will try to go forward and turn a bit on the left to try to find wall to follow on his left side\n",
    "2) If there is an obstacle close enough: \n",
    "\n",
    "    2.1) If the Thymio is really close to the obstalce, it will move backward\n",
    "    \n",
    "    2.2) It will move to the right trying avoiding the obstacle\n",
    "\n",
    "Depending if the sensor is the frontal one, or the sides ones he will have a similar reaction but more or less accentuated\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792dc48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.local_navigation import LocalNavigation\n",
    "localNav = LocalNavigation(ctx, MotionControl)\n",
    "\n",
    "localNav.process_event(variables={\"prox.horizontal\": [2800,3000,3300,3000,3400,2000,2000]}) #simulate the sensors \n",
    "distArray = localNav.getDistanceArray() #the distance array to define the real distance between the sensor and the obstacle\n",
    "ctx.state.relative_distances = distArray\n",
    "print(distArray[:-2])\n",
    "(arrived, vLC, vRC) = control.controlWithDistance()\n",
    "print(vLC, vRC)\n",
    "print(\"The Thymio es going foward and turning a bit on the left to try figure out where is the wall \")\n",
    "\n",
    "localNav.process_event(variables={\"prox.horizontal\": [2800,3000,4500,3000,3400,2000,2000]}) #simulate the sensors \n",
    "distArray = localNav.getDistanceArray() #the distance array to define the real distance between the sensor and the obstacle\n",
    "ctx.state.relative_distances = distArray\n",
    "\n",
    "print(distArray[:-2])\n",
    "(arrived, vLC, vRC) = control.controlWithDistance()\n",
    "print(vLC, vRC)\n",
    "print(\"The Thymio es going backward and turning on the left to avoid the wall\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "54757940",
   "metadata": {},
   "source": [
    "In our program, we are calling the funtion 'update_motor_control()' this function chooses the correct control defined in the Local Navigation Block. And also he will update the waypoint if it arrives to the previous waypoint"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8976982e",
   "metadata": {},
   "source": [
    "## <a id='toc1_8_'></a>[Local navigation](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "f4c039f3",
   "metadata": {},
   "source": [
    "The block Local navigation has the responsability to trigger the control by waypoint or the control with distance depending if the Thymio senses an obstacle close to his sensors. He communicates the type of control threw the state variable : 'reactive_control'. \n",
    "\n",
    "This block can also detect and locate the obstacle position with the function: 'getWallRelative()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82911935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.local_navigation import LocalNavigation\n",
    "\n",
    "localNav = LocalNavigation(ctx, MotionControl)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d51e7bc7",
   "metadata": {},
   "source": [
    "This function doesn't exist in the project but for the notebook it is very useful because we can simulate a measure of the Thymio's sensors. And understand understand the differents functions. It updates the State"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658b5ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulateSensors(sensorsValues):\n",
    "    localNav.process_event(variables={\"prox.horizontal\": sensorsValues}) #simulate the sensors \n",
    "\n",
    "    distArray = localNav.getDistanceArray() #the distance array to define the distance [in cm] between the sensor and the obstacle\n",
    "    ctx.state.relative_distances = distArray "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec486f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulateSensors([4000,3000,3300,4000,3400,2000,2000])\n",
    "print(ctx.state.relative_distances)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "3b8140c3",
   "metadata": {},
   "source": [
    "The function 'getWallRelative()' returns the relative position of the sensed walls. I precise relative because it depends on the position and orientation of the Thymio. This function can be useful if in the futur we want to implemant a scan of obstacle with the Thymio "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf51a047",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulateSensors([3000,3000,3300,4000,3400,2000,2000])\n",
    "relative_ostacle_position = localNav.getWallRelative()\n",
    "print(relative_ostacle_position)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5c2498bb",
   "metadata": {},
   "source": [
    "Now we enter in the core of the Block, the trigger of the type of control depending on the sensors Values.\n",
    "The function 'should_freestyle()' defines if the Thymio should enter the reactive control mode and updates the State and will be read by the Motion Control block to apply the correct control \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8315a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx.state.reactive_control = False\n",
    "\n",
    "simulateSensors([3000,3000,3000,3000,3400,2000,2000]) #Read of sensors far away of an obstacle\n",
    "print(\"relative distances : \",ctx.state.relative_distances[:-2])\n",
    "localNav.should_freestyle()\n",
    "print(\"should enter local navigation ? : \",ctx.state.reactive_control)\n",
    "\n",
    "\n",
    "simulateSensors([3000,4500,4500,3000,3400,2000,2000])#Read of sensors close of an obstacle, the front one at 4500\n",
    "print(\"relative distances : \",ctx.state.relative_distances[:-2])\n",
    "localNav.should_freestyle()\n",
    "print(\"should enter local navigation ? : \",ctx.state.reactive_control)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37df7b0a",
   "metadata": {},
   "source": [
    "## <a id='toc1_9_'></a>[Putting it all together](#toc0_)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bf0736b7",
   "metadata": {},
   "source": [
    "Now that we have covered all the different modules of our application, we can put them together to run the whole thing.\n",
    "\n",
    "There are two ways to run the application.\n",
    "\n",
    "### Running from the terminal\n",
    "\n",
    "This is the **recommended** approach, it's more robust and allows you to see better application logs, stop the application with Ctrl+C and also use keyboard input using stdin.\n",
    "\n",
    "\n",
    "```powershell\n",
    "$ python -m app\n",
    "```\n",
    "\n",
    "\n",
    "### Running from Jupyter\n",
    "\n",
    "On modern versions of IPython (7+), it's possible to run the application from a Jupyter notebook using the top-level await syntax. From our experiments, the coloured terminal logging, GUI calibration window and keyboard input should all work.\n",
    "\n",
    "Note that running the following cell will block the notebook kernel until the cell is interrupted (the future is cancelled), the kernal is restarted or the Web UI is used to terminal the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e9b578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from asyncio import CancelledError\n",
    "from app.__main__ import init\n",
    "\n",
    "\n",
    "try:\n",
    "    await init()\n",
    "\n",
    "except CancelledError:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b97a24aec6f8def2674d93ea2437cb3728843830cb7d5b7fbd0461f275e8bc7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
