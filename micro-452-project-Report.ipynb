{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "92437df2",
   "metadata": {},
   "source": [
    "# <a id='toc1_'></a>[Mobile Robotics Project Report](#toc0_)\n",
    "\n",
    "---\n",
    "---\n",
    "Marcus Cemes, Pable Paller, Adrien Pannatier,  Carolina Rodrigues Fidalgo"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a741a9",
   "metadata": {},
   "source": [
    "\n",
    "<div style=\"border:1px solid black; padding:20px 20px;text-align: justify;text-justify: inter-word\">\n",
    "    This notebook describes the different parts of our project for the course micro-452: Basics of mobile robotics.<br/>\n",
    "    In the following cells we will describe how the modules work and give a structure to run the project.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f871766",
   "metadata": {},
   "source": [
    "**Table of contents**<a id='toc0_'></a>    \n",
    "- [Mobile Robotics Project Report](#toc1_)    \n",
    "  - [Prerequisites](#toc1_1_)    \n",
    "    - [Useful imports](#toc1_1_1_)    \n",
    "  - [Introduction](#toc1_2_)    \n",
    "    - [Architecture](#toc1_2_1_)    \n",
    "    - [Project specificities](#toc1_2_2_)    \n",
    "      - [The Signal class](#toc1_2_2_1_)    \n",
    "      - [The Context](#toc1_2_2_2_)    \n",
    "      - [The State](#toc1_2_2_3_)    \n",
    "        - [State patches](#toc1_2_2_3_1_)    \n",
    "      - [The Web UI](#toc1_2_2_4_)    \n",
    "  - [Vision](#toc1_3_)    \n",
    "  - [Filtering](#toc1_4_)    \n",
    "    - [Prediction](#toc1_4_1_)    \n",
    "    - [Update](#toc1_4_2_)    \n",
    "  - [Global navigation](#toc1_5_)    \n",
    "    - [Path optimisation](#toc1_5_1_)    \n",
    "  - [Big Brain](#toc1_6_)    \n",
    "    - [Significant updates](#toc1_6_1_)    \n",
    "    - [Christmas animation](#toc1_6_2_)    \n",
    "  - [Motion control](#toc1_7_)    \n",
    "  - [Local navigation](#toc1_8_)    \n",
    "  - [Putting it all together](#toc1_9_)    \n",
    "    - [Running from the terminal](#toc1_9_1_)    \n",
    "    - [Running from Jupyter](#toc1_9_2_)    \n",
    "\n",
    "<!-- vscode-jupyter-toc-config\n",
    "\tnumbering=false\n",
    "\tanchor=true\n",
    "\tflat=false\n",
    "\tminLevel=1\n",
    "\tmaxLevel=6\n",
    "\t/vscode-jupyter-toc-config -->\n",
    "<!-- THIS CELL WILL BE REPLACED ON TOC UPDATE. DO NOT WRITE YOUR TEXT IN THIS CELL -->"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71170441",
   "metadata": {},
   "source": [
    "## <a id='toc1_1_'></a>[Prerequisites](#toc0_)\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Tip:</b> In the latest Python/Jupyter version, it should no longer be necsesary to add our application module to the path. If you experience any import errors, convert the following block to code and run it before anything else.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "raw",
   "id": "1c264085-9b4e-4ab4-b916-cdce9283d090",
   "metadata": {},
   "source": [
    "from sys import path\n",
    "path.append(\"./app\")\n",
    "path.append(\"./report\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d1ebe28-e5a2-432d-9f98-9125c918c382",
   "metadata": {},
   "source": [
    "This project has been tested on **Python 3.10** and **JupyterLab 3.5**, allowing for features such as:\n",
    "\n",
    "- Python's new `match` statement\n",
    "- Top-level async/await\n",
    "- Integrated event loop in the notebook\n",
    "\n",
    "To make sure that your environment is supported, try running the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c431f131",
   "metadata": {},
   "outputs": [],
   "source": [
    "from report.utils import run_checks\n",
    "\n",
    "run_checks()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58b29f90",
   "metadata": {},
   "source": [
    "If you have any missing requirements, convert the following into a code block and run it (a [venv](https://docs.python.org/3/library/venv.html) is recommended)."
   ]
  },
  {
   "cell_type": "raw",
   "id": "607d87d6-0b91-451f-8d29-d70634d9ad8d",
   "metadata": {},
   "source": [
    "!pip3 install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2ed8216",
   "metadata": {},
   "source": [
    "Additional installation instructions can be found in the project's [README.md](README.md).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4558a53",
   "metadata": {},
   "source": [
    "### <a id='toc1_1_1_'></a>[Useful imports](#toc0_)\n",
    "\n",
    "We use these libraries and imports throughout the notebook, run this cell to make them available."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2a95436-6c5a-4c20-a856-f09d96e45597",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "from asyncio import create_task, sleep\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from app.config import *\n",
    "\n",
    "from report.map import *\n",
    "from report import report_functions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "334365db",
   "metadata": {},
   "source": [
    "___\n",
    "## <a id='toc1_2_'></a>[Introduction](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5bdf3240",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_1_'></a>[Architecture](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0973c3db-1314-41e3-825d-27e2f5f5c546",
   "metadata": {},
   "source": [
    "Our project is composed by 8 different modules presented in the following figure:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "738eb0d9-9262-4afe-8a98-cb2e187d1a34",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"padding: 1rem 0;\">\n",
    "    <figure>\n",
    "        <img src=\"assets/report/Project_scheme.jpg\" style=\"width: 40rem;\" />\n",
    "        <figcaption style=\"font-style: italic;\">Communication between the modules of the project.</figcaption>\n",
    "    </figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35906d49-ccf4-4cfc-92e2-4502d1ce30fd",
   "metadata": {},
   "source": [
    "All modules are independant and interact with one another sharing a shared [Context class] \n",
    "\n",
    "Some modules use [signals] to react to certain events and thus be independant from timers.\n",
    "\n",
    "The main module is [Big brain] which processes information from other modules and give orders."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a45df129-021a-43fb-83ab-f19475e28ffb",
   "metadata": {},
   "source": [
    "1. Classes\n",
    "2. Single instaniation\n",
    "3. Shared context\n",
    "4. Signals vs. composition\n",
    "5. Module class inheritence\n",
    "6. Web UI + control server"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "613c8aa3-c166-4f70-b3b0-96e6838b2336",
   "metadata": {},
   "source": [
    "But before going into detail about the modules. Let's focus on the specificities of our project:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127de18f",
   "metadata": {},
   "source": [
    "### <a id='toc1_2_2_'></a>[Project specificities](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84fc15a0",
   "metadata": {},
   "source": [
    "#### <a id='toc1_2_2_1_'></a>[The Signal class](#toc0_)\n",
    "\n",
    "The `Signal` class ([types.py](./app/utils/types.py))  is a custom asyncronous syncronisation primitive. It wraps `asyncio`'s `Event` object, exposing two methods:\n",
    "\n",
    "- `async wait(timeout: float)`\n",
    "- `trigger()`\n",
    "\n",
    "When triggered, it will allow all other tasks that are `await`ing the signal to continue execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c430ab1d-6e38-41e2-919a-ab229cb52974",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.utils.types import Signal\n",
    "\n",
    "signal = Signal()\n",
    "\n",
    "async def wait_for_notification(id):\n",
    "    print(f\"Task {id} is waiting...\")\n",
    "    await signal.wait()\n",
    "    print(f\"Task {id} has been notified!\")\n",
    "\n",
    "# Create two tasks that will wait for the trigger\n",
    "for i in range(3):\n",
    "    create_task(wait_for_notification(i))\n",
    "\n",
    "await sleep(2)\n",
    "signal.trigger()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fddc29d5-8447-4948-b307-d6227b1df60a",
   "metadata": {},
   "source": [
    "We use signals extensively to asyncronously communicate between modules when certain events occur, without any dependence between our classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c2aecb",
   "metadata": {},
   "source": [
    "#### <a id='toc1_2_2_2_'></a>[The Context](#toc0_)\n",
    "\n",
    "One of the key elements in our project architecture is the `Context` class ([context.py](./app/context.py)). It allows us to to have a single shared point of access to important data, such as the mutable application state or to the connected Thymio nodes. It avoids the use of any global variables, and also allows us to create a simple mock interface for this notebook for demonstration purposes.\n",
    "\n",
    "Let's create one now that we will use for the rest of the report."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852ae7e7-d275-4a25-8105-792fff7fdf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.context import Context\n",
    "from app.state import State\n",
    "\n",
    "from report.mock_pool import MockPool\n",
    "\n",
    "ctx = Context(node=None, node_top=None, pool=MockPool(), state=State())\n",
    "\n",
    "# Set configuration attributes from constants\n",
    "ctx.state.physical_size = PHYSICAL_SIZE_CM\n",
    "ctx.state.subdivisions = SUBDIVISIONS\n",
    "\n",
    "pprint(ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90852f6f-41df-4731-a6ab-eab33b70955b",
   "metadata": {},
   "source": [
    "It contains a few signals, that are used to decouple our modules from each other whilst allowing them to communicate events, the Thymio nodes a shared state object and a few other items.\n",
    "\n",
    "| **Key**          | **Type**       | **Description**                                                            |\n",
    "|------------------|----------------|----------------------------------------------------------------------------|\n",
    "| **node**         | `Node`         | The primary Thymio node                                                    |\n",
    "| **node_top**     | `Node \\| None` | An optinal secondary Thymio node                                           |\n",
    "| **state**        | `State`        | The singleton `State` instance                                             |\n",
    "| **pool**         | `Pool`         | A ProcessPoolExecutor for offloading CPU-intensive work to another process |\n",
    "| **scene_update** | `Signal`       | A signal to indicate that the map has been changed                         |\n",
    "| **pose_update**  | `Signal`       | A signal to indicate that the robot has moved                              |\n",
    "| **debug_update** | `boolean`      | Queue a single iteration of vision image processing debugging (GUI)        |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb1f28ce",
   "metadata": {},
   "source": [
    "#### <a id='toc1_2_2_3_'></a>[The State](#toc0_)\n",
    "\n",
    "The `State` class was originally created for a single purpose: simplify the update process to the Web UI. The `State` class internally has a `Signal` that is used to notify other tasks that the state has been modified, propagating the changes to all connected WebSocket clients over the control server.\n",
    "\n",
    "**Rule of thumb**: anything that is to be viewed on the Web UI should belong in state. Anything else can be a private class attribute.\n",
    "\n",
    "##### <a id='toc1_2_2_3_1_'></a>[State patches](#toc0_)\n",
    "\n",
    "The control server creates a `ChangeListener` for each WebSocket connection, which listens for state changes and creates a minimum patch from the last call. In the following demo, we create three listeners, each with variying amounts of \"busyness\". Each task is able to independently track changes to reach a consistent version of the state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "288a034d-dd0d-45b4-a5e7-3015ca4f4f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from report.introduction import scenario_state_patches\n",
    "\n",
    "await scenario_state_patches()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57ef829a-580f-4abf-af39-a853cba6500c",
   "metadata": {},
   "source": [
    "> Understanding the implementation is not vital for this project. It allows for each WebSocket connection to have its own `ChangeListener`, with it's own dictionary of changes since the last call to `get_patch()`, ensuring that even if multiple clients are connected simultaneously, they can receive constient state updates. When a WebSocket connection is first made, the entire `State` is serialised and sent to the browser. This assited us in collaborative development and debugging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22eb88e0",
   "metadata": {},
   "source": [
    "#### <a id='toc1_2_2_4_'></a>[The Web UI](#toc0_)\n",
    "\n",
    "To interact with our running application and to render a detailed map of detected obstacles, we developped a web application that connects to the Python application's control server on port 8080.\n",
    "\n",
    "It's use is not mandatory for this report, but can be used to connect to the application when it's run in its entirety at the end of this report. For more information, see the [README.md](./README.md). It requires a recent version of [Node.jsÂ®](https://nodejs.org) to be installed on your system.\n",
    "\n",
    "```powershell\n",
    "$ cd ui\n",
    "$ npm install\n",
    "$ npm run build\n",
    "$ npm run preview\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2eebe37-373e-490c-9487-f6958201d6b0",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"padding: 1rem 0;\">\n",
    "    <figure>\n",
    "        <img src=\"assets/report/ui.png\" style=\"width: 60rem;\" />\n",
    "        <figcaption style=\"font-style: italic;\">A screenshot of the Web UI with its various options</figcaption>\n",
    "    </figure>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f7eb1ed",
   "metadata": {},
   "source": [
    "For the purposes of this report, we have developed a simplified view of the map that makes use of `matplotlib`. Throughout the report, we will make use of the plotting function to visualise the application state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "960784ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_map(ctx, \"Example map of the current state\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc027f9",
   "metadata": {},
   "source": [
    "___\n",
    "## <a id='toc1_3_'></a>[Vision](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ffaf769",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.vision import Vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "455561c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision = Vision(ctx, external=False, live=False, image_path=\"assets/test_frame_01.jpg\")\n",
    "reset(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc89670d",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision.calibrate();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "498a15ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = vision.next()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2938f9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_pose = (obs.back[0], obs.back[1], report_functions.angle(obs.back, obs.front))\n",
    "print(vision_pose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "939b1fc9",
   "metadata": {},
   "source": [
    "display the vision retranscription"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33b79a0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx.state.position = (obs.back[0], obs.back[1])\n",
    "ctx.state.orientation = report_functions.angle(obs.back, obs.front)\n",
    "ctx.state.obstacles = obs.obstacles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ed55e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_map(ctx, \"vision retranscription\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b89e0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "show_picture(\"assets/test_frame_01.jpg\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c35535a",
   "metadata": {},
   "source": [
    "___\n",
    "## <a id='toc1_4_'></a>[Filtering](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9184e358",
   "metadata": {},
   "source": [
    "Another important module of our project is filtering. Filtering is taking the information given by sensors and the camera to be able to guess where the thymio is on the map and where it's going.</br>\n",
    "For this project we decieded to use an [Extanded Kalman Filter](app/EXF.py) and an intermediate module called [filtering](app/filtering.py) to comunicate with the main module.</br>\n",
    "The EKF was chosen for its capability of predicting a non-linear evolution of the state and also to be able to dissociate information coming from the motors and information given by the camera.</br>\n",
    "For this purpose, two functions where used: [predict](#toc1_4_1_) and [update](#toc1_4_2_). We will come back to these parts in the subsections below.\n",
    "___\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8262ce5f",
   "metadata": {},
   "source": [
    "first let's import the module "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a783d006-43b1-41db-9b4d-cea07e80d151",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.filtering import Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79198013",
   "metadata": {},
   "outputs": [],
   "source": [
    "#run this cell to reset this section\n",
    "#creation of the filter\n",
    "filtering = Filtering(ctx)\n",
    "reset(ctx)\n",
    "report_functions.start_movement_simulation(filtering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "724c6334",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_1_'></a>[Prediction](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0727adbe",
   "metadata": {},
   "source": [
    "A specificity of our project is the time delay between camera and motor sensors. Since the motor sensors response is faster than the one from vision, what we do is predict the position of the robot with the motor sensors at a greater frequency than the update with vision information. The time interval between each prediction is dynamic and adapts to correctly predict each time the motor sensors send variables.\n",
    "\n",
    "The next position and orientation are then predicted.\n",
    "\n",
    "Let us run it to see how it works\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64eea28e-738a-4ad9-9c3f-c5c355245e14",
   "metadata": {},
   "source": [
    "Let's take a look at our current position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17073ea1",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_functions.print_pose(ctx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca6efbbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_map(ctx, \"Filtering Map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1e8ca65-a9b1-4899-a710-2d010667779a",
   "metadata": {},
   "source": [
    "To execute a filtering step let's make the Thymio move forward:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7764f84c-156b-4b85-ae62-e9dec4ff37e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering.process_event({\"motor.left.speed\": [70], \"motor.right.speed\": [70]})\n",
    "report_functions.stop_movement_simulation(filtering)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ca19d6-98c8-48a9-9395-84acfb11daf6",
   "metadata": {},
   "source": [
    "The state should now be updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c973de18-b444-4774-ac4e-f7f84117f772",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_functions.print_pose(ctx)\n",
    "plot_map(ctx, \"Filtering Map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f19687a",
   "metadata": {},
   "source": [
    "Let's do it again, turning the robot this time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c5ccfa1",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtering.process_event({\"motor.left.speed\": [50], \"motor.right.speed\": [-50]})\n",
    "report_functions.stop_movement_simulation(filtering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98537fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_functions.print_pose(ctx)\n",
    "plot_map(ctx, \"Filtering Map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64d77519",
   "metadata": {},
   "source": [
    "we see that filtering is correctly predicting the progression of the simulated robot"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5915e98",
   "metadata": {},
   "source": [
    "### <a id='toc1_4_2_'></a>[Update](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed29ff62",
   "metadata": {},
   "source": [
    "To reduce the error caused by motors, vision must be used as much as possible. If vision is lost for a while, the update step brings back the Thymio to it's correct position taking into consideration the computed prediction since it's disapearing.\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c5211d6",
   "metadata": {},
   "source": [
    "now to display the update function, we use the image and position of the robot given by the [Vision module](#toc1_3_)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2208520",
   "metadata": {},
   "source": [
    "The position after our prediction step is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "155ea781",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_functions.print_pose(ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12e43b13",
   "metadata": {},
   "source": [
    "now we update the position with the one given by vision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea0726a",
   "metadata": {},
   "outputs": [],
   "source": [
    "vision_position = vision #get image from vision\n",
    "filtering.update(vision_pose)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb21370f",
   "metadata": {},
   "source": [
    "after the vision update, the new position is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b098a5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_functions.print_pose(ctx)\n",
    "plot_map(ctx, \"Filtering Map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c100d767",
   "metadata": {},
   "source": [
    "___\n",
    "## <a id='toc1_5_'></a>[Global navigation](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32fa2c9a-b7e0-4a09-8205-5c5f14549425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.global_navigation import GlobalNavigation\n",
    "\n",
    "global_nav = GlobalNavigation(ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20272c96-5932-449e-bd7e-0aa6dd99bbdb",
   "metadata": {},
   "source": [
    "_Source: [global_navigation.py](./app/global_navigation.py)_\n",
    "\n",
    "At the moment our map is completely empty. In order to invoke our pathfinding algorithm, we need to set the Thymio's position (<b style=\"color: red;\">red</b>) and a desired end point (<b style=\"color: cyan;\">cyan</b>)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "031257b3-5db9-4211-a549-bbf6ca99c3c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start with a fresh new map\n",
    "reset(ctx)\n",
    "\n",
    "# Update the state\n",
    "ctx.state.position = (10, 10)\n",
    "ctx.state.end = (90, 110)\n",
    "\n",
    "# Recompute the path\n",
    "await global_nav._recompute_path()\n",
    "\n",
    "# Plot the result\n",
    "plot_map(ctx, \"Path-finding map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1862c56-0290-462f-bb49-6cf72c3c6be1",
   "metadata": {},
   "source": [
    "**ðŸŽ‰ We have a path!**\n",
    "\n",
    "Let's not celebrate yet, we can make this much more interesting. Our pathfinding uses an implementation of the [Dijkstra's algorithm](https://en.wikipedia.org/wiki/Dijkstra%27s_algorithm). While this may not be the most fastest or most efficient algorithm, it _does_ guarantee optimal results for a given graph.\n",
    "\n",
    "Our node graph is a grid of equaly spaced cells, where edges are made between directly adjacent cells and diagonal cells, with a cost of 1 and $\\sqrt{2}$, respectively. This allows our robot to also travel diagonally and avoid grid-like movement.\n",
    "\n",
    "<div align=\"center\" style=\"padding: 1rem 0;\">\n",
    "    <figure>\n",
    "        <img src=\"assets/report/dijkstra-nodes.png\" style=\"width: 8rem;\" />\n",
    "        <figcaption style=\"font-style: italic;\">Edges for a graph node to its adjacent nodes.</figcaption>\n",
    "    </figure>\n",
    "</div>\n",
    "\n",
    "Ideally, we would like all nodes to be connected to every other node, given that there is a line-of-sight between them, however this comes with a huge computational burden. This simple implementation (mixed with our path optimisation algorithm covered later) provides adequate results in roughly 50 ms for 64 subdivisions.\n",
    "\n",
    "Time for some obstacles! Obstacles come directly from the vision module as a matrix of 8-bit integers, where any value other an 0 represent an occupied cell. The size of the matrix coincides with the node graph, an occupied cell will mark a node as unvisitable.\n",
    "\n",
    "**Note:** While we could have also used a matrix of booleans, integers are slightly easier to work with and compose with other functions such as convolutions. Additionally, it has a smaller JSON serialisation footprint for sending over WebSockets to the Web UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ce68b7b-0da6-4a1f-9831-4164dcae34fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the matrix\n",
    "ctx.state.obstacles[:,:] = 0\n",
    "\n",
    "# Add two obstacles\n",
    "ctx.state.obstacles[30:45, 40:55] = 1\n",
    "ctx.state.obstacles[20:30, 5: 20] = 1\n",
    "\n",
    "# Recompute the path\n",
    "await global_nav._recompute_path()\n",
    "\n",
    "# Plot the result\n",
    "plot_map(ctx, \"Path-finding map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a41ad200-eee1-4a3d-9148-760c033a7b21",
   "metadata": {},
   "source": [
    "The dark-blue region represents the obstacles that we set. The cyan regions are the obstacle boundaries that are generated using a convolution of a circular kernel matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8f01aae-80a6-4476-b9dd-287c0007d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "kernel = global_nav._safety_margin_kernel()\n",
    "\n",
    "plot_image(kernel, \"Obstacle boundary convolution kernel\", colourbar=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "916d9a38-5988-44e6-a868-db8a058f38e0",
   "metadata": {},
   "source": [
    "The boundary is equaly treated as unvisitable by the pathfinding algorith. This ensures that the Thymio does not hit any objects with its wheels as it's navigating the path. It a gap is too small, the Thymio will not try to fit through it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cc68a70-6455-42bf-a33f-f6299aeeb3cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the matrix\n",
    "ctx.state.obstacles[:,:] = 0\n",
    "\n",
    "# Add two obstacles with a gap\n",
    "ctx.state.obstacles[31:33, 15:29] = 1\n",
    "ctx.state.obstacles[31:33, 42:50] = 1\n",
    "\n",
    "# Recompute the path\n",
    "await global_nav._recompute_path()\n",
    "\n",
    "# Plot the result\n",
    "plot_map(ctx, \"Path-finding map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3443b6aa",
   "metadata": {},
   "source": [
    "### <a id='toc1_5_1_'></a>[Path optimisation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb8a7fc5-bff9-43ee-a8fd-d9624d349191",
   "metadata": {},
   "source": [
    "Our chosen node graph constraints our robots movements more or less to a grid, with the ability of also being able to move diagonally. This creates a lot of path waypoints, but also creates less natural-like movement.\n",
    "\n",
    "To combat this, we have an additional post-pathfinding step to iteratively try to reduce the path to as few waypoints as necessary that have free line of sight between them. This does not find the most optimal solution, but the results are more than adequate.\n",
    "\n",
    "The algorithm goes as follows:\n",
    "\n",
    "1. For $i = 2,...,M-1$, where $M$ is the number of waypoints\n",
    "2. Check whether there is a free line-of-sight between waypoints $w_{i-1}$ and $w_{i+1}$\n",
    "3. If yes, remove the waypoint $w_i$, otherwise consider the next waypoint $w_{i+1}$\n",
    "\n",
    "In order to calculate free line-of-sight, it's necessary to enumerate all map cells that the segment between $w_{i-1}$ and $w_{i+1}$ travels through to check their occupation status. This is done using an algorithm such as [Bresenham's line algorithm](https://en.wikipedia.org/wiki/Bresenham%27s_line_algorithm). We found an improved version that also uses integer-only math [here](https://playtechs.blogspot.com/2007/03/raytracing-on-grid.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bb9c40-6fda-40c1-8730-e0a7f0f7c175",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.path_finding.path_optimiser import raytrace\n",
    "\n",
    "p1 = (1, 2)\n",
    "p2 = (8, 6)\n",
    "\n",
    "matrix = np.zeros((10, 10))\n",
    "\n",
    "for (i, j) in raytrace(p1, p2):\n",
    "    matrix[j][i] = 1\n",
    "\n",
    "plot_raytrace(matrix, p1, p2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e684f7c-1e87-4298-ad7e-3fd33f89b28b",
   "metadata": {},
   "source": [
    "Let's put this into practice using the path that we found previously."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a91d176-ac3b-42e9-88c7-175818e8c808",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.path_finding.path_optimiser import PathOptimiser\n",
    "\n",
    "map = create_map(ctx)\n",
    "\n",
    "optimiser = PathOptimiser(map)\n",
    "\n",
    "# The output of the pathfinding algorithm is in physical space [cm],\n",
    "# it needs to be converted back into map indicies for this demonstration.\n",
    "# Usually the path optimisation is done as an intermediate step by the\n",
    "# path finding algorithm, before the conversion to physical space.\n",
    "path = path_to_coords(ctx.state.path)\n",
    "\n",
    "# The optimiser mutates the input list directly\n",
    "optimised_path = optimiser.optimise(path.copy())\n",
    "\n",
    "plot_path_optimisation(path, optimised_path, map)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1ad1fd-2f4a-4207-b794-703dd6b9daa9",
   "metadata": {},
   "source": [
    "Path optimisation can be toggled dynamically by setting the `ctx.state.optimise` boolean flag."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fca233e2-1175-489f-96dc-6dee946cdac6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable path optimisation\n",
    "ctx.state.optimise = True\n",
    "\n",
    "# Reset the matrix\n",
    "ctx.state.obstacles[:,:] = 0\n",
    "\n",
    "# Add two obstacles\n",
    "ctx.state.obstacles[30:45, 40:55] = 1\n",
    "ctx.state.obstacles[20:30, 5: 20] = 1\n",
    "\n",
    "await global_nav._recompute_path()\n",
    "\n",
    "plot_map(ctx, \"Path-finding map\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5f3f1d2",
   "metadata": {},
   "source": [
    "___\n",
    "## <a id='toc1_6_'></a>[Big Brain](#toc0_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd21e289",
   "metadata": {},
   "outputs": [],
   "source": [
    "import app.big_brain\n",
    "\n",
    "big_brain = app.big_brain.BigBrain(ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a937484f",
   "metadata": {},
   "source": [
    "_Source: [big_brain.py](app/big_brain.py)_\n",
    "\n",
    "The **Big Brain** module is in charge of the main application logic. It's purpose is to initialise the other modules and to orchestrate the flow of data between them.\n",
    "\n",
    "The submodules are able to run autonomously in the background of the application using Python's `with` statement and the `asyncio` library. The `__enter__` and `__exit__` methods are used to initialise and clean up the resources used by the submodules, such as long running tasks or registering and unregistering Thymio event handlers.\n",
    "\n",
    "Once the submodules are initialised, the main loop of the application is started. This loop is responsible for acquiring new data from the vision module and providing this data to the filtering and global navigation modules, which then trickle down to motion control and to the Web UI.\n",
    "\n",
    "### <a id='toc1_6_1_'></a>[Significant updates](#toc0_)\n",
    "\n",
    "To reduce the number of map updates from the vision module, we have implemented a simple thresholding function that checks whether the $L_1$ norm between the current and previous map is greater than a given threshold (i.e. a certain number of grid squares have changed). This reduces redundant pathfinding computation and also renders motion control more stable, as it is less likely to be interrupted by a sudden change in the map and path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c56636e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx.state.obstacles = np.zeros((SUBDIVISIONS, SUBDIVISIONS), dtype=np.int8)\n",
    "new_map = np.zeros((SUBDIVISIONS, SUBDIVISIONS), dtype=np.int8)\n",
    "\n",
    "# Changing a signal square does not report a significant change\n",
    "new_map[1, 1] = 1\n",
    "\n",
    "big_brain.significant_change(new_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d36e30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Changing more than the threshold of squares should report a significant change\n",
    "new_map[1:5, 1:5] = 1\n",
    "\n",
    "big_brain.significant_change(new_map, threshold=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed9f14f0",
   "metadata": {},
   "source": [
    "If the new map is deemed to have a significant change, the obstacle map is updated in `ctx.state` and the `ctx.scene_update` signal is triggered, which in turn triggers the global navigation module to recompute the path.\n",
    "\n",
    "Regardless of whether the map has changed or not, the new detected position is sent to the filtering module in order to update the robot's position and attempt to correct any drift.\n",
    "\n",
    "### <a id='toc1_6_2_'></a>[Christmas animation](#toc0_)\n",
    "\n",
    "Once the robot reaches the arrival point, it will start to play a celebration sequence. This is also handled by the big brain module, invoking the `ChristmasCelebration` class to play various actions, such as dropping the bauble using the second Thymio node."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6794ddb",
   "metadata": {},
   "source": [
    "___\n",
    "## <a id='toc1_7_'></a>[Motion control](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ecf9330",
   "metadata": {},
   "source": [
    "The block motion control has the responsability to move the thymio. It has 2 types of control, positional control 'controlPosition()' which has the objective to go to a special set of coordinates. And reactive control 'controlWithDistance()' which has the objective to avoid any walls and get around them following their left.\n",
    "\n",
    "Motion control also has the responsability to update the targeted waypoint if it is reached and commmunicate to the rest of the program if the last waypoint is reached. \n",
    "\n",
    "1) Update the waypoint when arrived\n",
    "2) If arrived to the end of the path, change the state to let the rest of the program know "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7717523",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.motion_control import MotionControl\n",
    "#creation of the filter\n",
    "control = MotionControl(ctx)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05f1e13d",
   "metadata": {},
   "source": [
    "Let's simulate a path following !\n",
    "\n",
    "SETUP :\n",
    "1) We have do define a path (noramlly given by BigBrain in Global-nav)\n",
    "2) We have to define which waypoint we are aiming\n",
    "\n",
    "\n",
    "SIMULATION : \n",
    "1) We control the Thymio to go to the specified waypoint we want \n",
    "2) We update his position by updating filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97c97dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "## SETUP\n",
    "#1)\n",
    "ctx.state.boundary_map = None\n",
    "ctx.state.obstacles = None\n",
    "report.map.reset(ctx)\n",
    "\n",
    "ctx.state.path = [[10, 0], [10, 10], [20, 10], [0,0], [0,0]] #the path we want to follow\n",
    "ctx.state.position = [0,0]\n",
    "ctx.state.orientation = 0\n",
    "filtering.ekf.__init__(ctx.state.position, ctx.state.orientation) #let's force the Thymio position to [0,0] and an orientation of 0\n",
    "ctx.state.next_waypoint_index = 0 # we want to have the first waypoint \n",
    "ctx.state.reactive_control = False\n",
    "\n",
    "\n",
    "#2)\n",
    "control.setNewWaypoint(0) #load the next waypoint we want to go to\n",
    "print(\"objective waypoint : \",control.waypoint) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f6beff0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from IPython.display import clear_output\n",
    "\n",
    "while(ctx.state.next_waypoint_index < len(ctx.state.path)-1):\n",
    "    clear_output(wait=True)\n",
    "    report.map.plot_map(ctx, \"Path-finding map\")\n",
    "    \n",
    "    (arrived, vLC, vRC) = control.controlPosition() #1)Get control\n",
    "    if(arrived):\n",
    "        control.setNewWaypoint(1) #load the next waypoint to aim if arrived to the previous one\n",
    "    \n",
    "    filtering.process_event({\"motor.left.speed\": [int(vLC)], \"motor.right.speed\": [int(vRC)]}) #    2)force an update of position \n",
    "    time.sleep(0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1672905",
   "metadata": {},
   "source": [
    "We need to introduce the Local Navigation block to understand completely the 'controlWithDistance()' function. The full explaination can be found in the [local navigation section](#toc1_8_).\n",
    "\n",
    "Motion control receives the distance array which contains how far from the obstacle the Thymio is.\n",
    "\n",
    "Distance control is very simple:\n",
    "\n",
    "1) If no obstacles : Thymio will try to go forward and turn a bit to the right trying to find wall to follow on his leften side\n",
    "2) If an obstacle is close enough: \n",
    "\n",
    "    2.1) If the Thymio is really close to the obstalce, it will move backward\n",
    "    \n",
    "    2.2) It will move to the right trying to avoid the obstacle\n",
    "\n",
    "Depending if the triggered sensor is frontal or from the sides, the reaction will be similar but more or less accentuated.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "792dc48f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.local_navigation import LocalNavigation\n",
    "localNav = LocalNavigation(ctx, MotionControl)\n",
    "\n",
    "localNav.process_event(variables={\"prox.horizontal\": [2800,3000,3300,3000,3400,2000,2000]}) #simulate the sensors \n",
    "distArray = localNav.getDistanceArray() #the distance array to define the real distance between the sensor and the obstacle\n",
    "ctx.state.relative_distances = distArray\n",
    "print(distArray[:-2])\n",
    "(arrived, vLC, vRC) = control.controlWithDistance()\n",
    "print(vLC, vRC)\n",
    "print(\"The Thymio is going foward and turning a bit to the right to try figure out where the wall is \")\n",
    "\n",
    "localNav.process_event(variables={\"prox.horizontal\": [2800,3000,4500,3000,3400,2000,2000]}) #simulate the sensors \n",
    "distArray = localNav.getDistanceArray() #the distance array to define the real distance between the sensor and the obstacle\n",
    "ctx.state.relative_distances = distArray\n",
    "\n",
    "print(distArray[:-2])\n",
    "(arrived, vLC, vRC) = control.controlWithDistance()\n",
    "print(vLC, vRC)\n",
    "print(\"The Thymio is going backward and turning to the left to avoid the wall\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54757940",
   "metadata": {},
   "source": [
    "In our program, we are calling the funtion 'update_motor_control()' this function chooses the correct control defined in the Local navigation block. The function will also update the targeted waypoint if the thymio arrives to the waypoint"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d57273e9",
   "metadata": {},
   "source": [
    "___\n",
    "## <a id='toc1_8_'></a>[Local navigation](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4c039f3",
   "metadata": {},
   "source": [
    "The Local navigation block has the responsability to trigger the control by waypoint or the control with distance state depending if the Thymio senses an obstacle close to his sensors. It communicates the type of control through the state variable : 'reactive_control'. \n",
    "\n",
    "This block can also detect and locate obstacle positions with the function: 'getWallRelative()'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82911935",
   "metadata": {},
   "outputs": [],
   "source": [
    "from app.local_navigation import LocalNavigation\n",
    "\n",
    "localNav = LocalNavigation(ctx, MotionControl)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d51e7bc7",
   "metadata": {},
   "source": [
    "This function does not exist in the project but for the report it is very useful because we can simulate a measure of the Thymio's sensors and update the State."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "658b5ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simulateSensors(sensorsValues):\n",
    "    localNav.process_event(variables={\"prox.horizontal\": sensorsValues}) #simulate the sensors \n",
    "\n",
    "    distArray = localNav.getDistanceArray() #the distance array to define the distance [in cm] between the sensor and the obstacle\n",
    "    ctx.state.relative_distances = distArray "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ec486f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulateSensors([4000,3000,3300,4000,3400,2000,2000])\n",
    "print(ctx.state.relative_distances)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b8140c3",
   "metadata": {},
   "source": [
    "The function 'getWallRelative()' returns the relative position of the sensed walls (relative because it depends on the position and orientation of the Thymio). This function could be useful if in the future we want to implement a scan of obstacles with the Thymio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf51a047",
   "metadata": {},
   "outputs": [],
   "source": [
    "simulateSensors([3000,3000,3300,4000,3400,2000,2000])\n",
    "relative_ostacle_position = localNav.getWallRelative()\n",
    "print(relative_ostacle_position)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c2498bb",
   "metadata": {},
   "source": [
    "Now we enter the main part of the block: the type of control trigger depending on the sensors Values.\n",
    "The function 'should_freestyle()' defines if the Thymio should enter the reactive control mode and updates the state. It will be read by the Motion control block for it to apply the correct control. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8315a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "ctx.state.reactive_control = False\n",
    "\n",
    "simulateSensors([3000,3000,3000,3000,3400,2000,2000]) #Reading of sensors far away from an obstacle\n",
    "print(\"relative distances : \",ctx.state.relative_distances[:-2])\n",
    "localNav.should_freestyle()\n",
    "print(\"should enter local navigation ? : \",ctx.state.reactive_control)\n",
    "\n",
    "\n",
    "simulateSensors([3000,4500,4500,3000,3400,2000,2000])#Reading of sensors close to an obstacle, the front one at 4500\n",
    "print(\"relative distances : \",ctx.state.relative_distances[:-2])\n",
    "localNav.should_freestyle()\n",
    "print(\"should enter local navigation ? : \",ctx.state.reactive_control)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05d3081e",
   "metadata": {},
   "source": [
    "___\n",
    "## <a id='toc1_9_'></a>[Putting it all together](#toc0_)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e234c1b6",
   "metadata": {},
   "source": [
    "Now that we have covered all the different modules of our application, we can put them together to run the whole thing.\n",
    "\n",
    "There are two ways to run the application.\n",
    "\n",
    "### <a id='toc1_9_1_'></a>[Running from the terminal](#toc0_)\n",
    "\n",
    "This is the **recommended** approach, it's more robust and allows you to see better application logs, stop the application with Ctrl+C and also use keyboard input using stdin.\n",
    "\n",
    "\n",
    "```powershell\n",
    "$ python -m app\n",
    "```\n",
    "\n",
    "\n",
    "### <a id='toc1_9_2_'></a>[Running from Jupyter](#toc0_)\n",
    "\n",
    "On modern versions of IPython (7+), it's possible to run the application from a Jupyter notebook using the top-level await syntax. From our experiments, the coloured terminal logging, GUI calibration window and keyboard input should all work.\n",
    "\n",
    "Note that running the following cell will block the notebook kernel until the cell is interrupted (the future is cancelled), the kernal is restarted or the Web UI is used to terminal the application."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e9b578",
   "metadata": {},
   "outputs": [],
   "source": [
    "from asyncio import CancelledError\n",
    "from app.__main__ import init\n",
    "\n",
    "\n",
    "try:\n",
    "    await init()\n",
    "\n",
    "except CancelledError:\n",
    "    pass"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "7b97a24aec6f8def2674d93ea2437cb3728843830cb7d5b7fbd0461f275e8bc7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
